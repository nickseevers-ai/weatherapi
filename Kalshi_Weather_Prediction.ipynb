{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nickseevers-ai/weatherapi/blob/main/Kalshi_Weather_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "NByHmXowBDX4",
        "outputId": "556a1bea-a22d-4dbd-ba61-a14200ef585c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting kalshi-python\n",
            "  Downloading kalshi_python-2.1.4-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: urllib3<3.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from kalshi-python) (2.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from kalshi-python) (2.9.0.post0)\n",
            "Requirement already satisfied: pydantic>=2 in /usr/local/lib/python3.12/dist-packages (from kalshi-python) (2.12.3)\n",
            "Requirement already satisfied: typing-extensions>=4.7.1 in /usr/local/lib/python3.12/dist-packages (from kalshi-python) (4.15.0)\n",
            "Collecting lazy-imports<2,>=1 (from kalshi-python)\n",
            "  Downloading lazy_imports-1.2.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2->kalshi-python) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2->kalshi-python) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2->kalshi-python) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->kalshi-python) (1.17.0)\n",
            "Downloading kalshi_python-2.1.4-py3-none-any.whl (181 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m181.4/181.4 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lazy_imports-1.2.0-py3-none-any.whl (18 kB)\n",
            "Installing collected packages: lazy-imports, kalshi-python\n",
            "Successfully installed kalshi-python-2.1.4 lazy-imports-1.2.0\n",
            "Authentication successful!\n"
          ]
        }
      ],
      "source": [
        "!pip install kalshi-python pandas\n",
        "\n",
        "import kalshi_python\n",
        "from kalshi_python import Configuration, KalshiClient\n",
        "from google.colab import userdata\n",
        "\n",
        "def get_client(environment=\"production\"):\n",
        "    # v2 URL works for all markets (Elections, Econ, etc.)\n",
        "    base_url = \"https://api.elections.kalshi.com/trade-api/v2\"\n",
        "    if environment == \"demo\":\n",
        "        base_url = \"https://demo-api.kalshi.co/trade-api/v2\"\n",
        "\n",
        "    config = Configuration(host=base_url)\n",
        "    config.api_key_id = userdata.get('KALSHI_KEY_ID')\n",
        "    config.private_key_pem = userdata.get('KALSHI_PRIVATE_KEY')\n",
        "\n",
        "    return KalshiClient(config)\n",
        "\n",
        "client = get_client(environment=\"production\")\n",
        "print(\"Authentication successful!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "3sihVWC7KC6S",
        "outputId": "7fc5b331-f7ec-4671-8dd2-538060e34dba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting cfgrib\n",
            "  Downloading cfgrib-0.9.15.1-py3-none-any.whl.metadata (56 kB)\n",
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/56.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.1/56.1 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting herbie-data[extras]\n",
            "  Downloading herbie_data-2026.1.1-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting eccodes>=2.45.0 (from herbie-data[extras])\n",
            "  Downloading eccodes-2.45.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting eccodeslib>=2.44.1.8 (from herbie-data[extras])\n",
            "  Downloading eccodeslib-2.45.1.9-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: numpy>=1.26.4 in /usr/local/lib/python3.12/dist-packages (from herbie-data[extras]) (2.0.2)\n",
            "Collecting pandas>=2.2.3 (from herbie-data[extras])\n",
            "  Downloading pandas-3.0.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (79 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyproj>=3.7.0 in /usr/local/lib/python3.12/dist-packages (from herbie-data[extras]) (3.7.2)\n",
            "Requirement already satisfied: requests>=2.23.3 in /usr/local/lib/python3.12/dist-packages (from herbie-data[extras]) (2.32.4)\n",
            "Requirement already satisfied: xarray>=2025.1.1 in /usr/local/lib/python3.12/dist-packages (from herbie-data[extras]) (2025.12.0)\n",
            "Collecting cartopy>=0.25.0 (from herbie-data[extras])\n",
            "  Downloading cartopy-0.25.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: matplotlib>=3.10.0 in /usr/local/lib/python3.12/dist-packages (from herbie-data[extras]) (3.10.0)\n",
            "Collecting metpy>=1.7.1 (from herbie-data[extras])\n",
            "  Downloading metpy-1.7.1-py3-none-any.whl.metadata (8.9 kB)\n",
            "Collecting scikit-learn>=1.8.0 (from herbie-data[extras])\n",
            "  Downloading scikit_learn-1.8.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: attrs>=19.2 in /usr/local/lib/python3.12/dist-packages (from cfgrib) (25.4.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from cfgrib) (8.3.1)\n",
            "Requirement already satisfied: shapely>=2.0 in /usr/local/lib/python3.12/dist-packages (from cartopy>=0.25.0->herbie-data[extras]) (2.1.2)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.12/dist-packages (from cartopy>=0.25.0->herbie-data[extras]) (26.0)\n",
            "Requirement already satisfied: pyshp>=2.3 in /usr/local/lib/python3.12/dist-packages (from cartopy>=0.25.0->herbie-data[extras]) (3.0.3)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.12/dist-packages (from eccodes>=2.45.0->herbie-data[extras]) (2.0.0)\n",
            "Collecting findlibs (from eccodes>=2.45.0->herbie-data[extras])\n",
            "  Downloading findlibs-0.1.2-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting eckitlib==1.32.4.9 (from eccodeslib>=2.44.1.8->herbie-data[extras])\n",
            "  Downloading eckitlib-1.32.4.9-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting fckitlib==0.14.1.9 (from eccodeslib>=2.44.1.8->herbie-data[extras])\n",
            "  Downloading fckitlib-0.14.1.9-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.1 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.10.0->herbie-data[extras]) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.10.0->herbie-data[extras]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.10.0->herbie-data[extras]) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.10.0->herbie-data[extras]) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.10.0->herbie-data[extras]) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.10.0->herbie-data[extras]) (3.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.10.0->herbie-data[extras]) (2.9.0.post0)\n",
            "Collecting pint>=0.17 (from metpy>=1.7.1->herbie-data[extras])\n",
            "  Downloading pint-0.25.2-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pooch>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from metpy>=1.7.1->herbie-data[extras]) (1.9.0)\n",
            "Requirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from metpy>=1.7.1->herbie-data[extras]) (1.16.3)\n",
            "Requirement already satisfied: traitlets>=5.1.0 in /usr/local/lib/python3.12/dist-packages (from metpy>=1.7.1->herbie-data[extras]) (5.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from pyproj>=3.7.0->herbie-data[extras]) (2026.1.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.3->herbie-data[extras]) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.3->herbie-data[extras]) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.3->herbie-data[extras]) (2.5.0)\n",
            "Requirement already satisfied: joblib>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.8.0->herbie-data[extras]) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.8.0->herbie-data[extras]) (3.6.0)\n",
            "Collecting flexcache>=0.3 (from pint>=0.17->metpy>=1.7.1->herbie-data[extras])\n",
            "  Downloading flexcache-0.3-py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting flexparser>=0.4 (from pint>=0.17->metpy>=1.7.1->herbie-data[extras])\n",
            "  Downloading flexparser-0.4-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: platformdirs>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from pint>=0.17->metpy>=1.7.1->herbie-data[extras]) (4.5.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from pint>=0.17->metpy>=1.7.1->herbie-data[extras]) (4.15.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.10.0->herbie-data[extras]) (1.17.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi->eccodes>=2.45.0->herbie-data[extras]) (3.0)\n",
            "Downloading cfgrib-0.9.15.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cartopy-0.25.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (11.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m77.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading eccodes-2.45.0-py3-none-any.whl (91 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m91.4/91.4 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading eccodeslib-2.45.1.9-cp312-cp312-manylinux_2_28_x86_64.whl (20.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m20.4/20.4 MB\u001b[0m \u001b[31m59.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading eckitlib-1.32.4.9-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (44.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.9/44.9 MB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fckitlib-0.14.1.9-cp312-cp312-manylinux_2_28_x86_64.whl (12.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m94.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading metpy-1.7.1-py3-none-any.whl (424 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m424.4/424.4 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-3.0.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (10.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_learn-1.8.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (8.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m89.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading herbie_data-2026.1.1-py3-none-any.whl (118 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m118.2/118.2 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pint-0.25.2-py3-none-any.whl (306 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m306.8/306.8 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading findlibs-0.1.2-py3-none-any.whl (10 kB)\n",
            "Downloading flexcache-0.3-py3-none-any.whl (13 kB)\n",
            "Downloading flexparser-0.4-py3-none-any.whl (27 kB)\n",
            "Installing collected packages: findlibs, eckitlib, flexparser, flexcache, fckitlib, scikit-learn, pint, pandas, eccodeslib, eccodes, cartopy, metpy, cfgrib, herbie-data\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.6.1\n",
            "    Uninstalling scikit-learn-1.6.1:\n",
            "      Successfully uninstalled scikit-learn-1.6.1\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 3.0.0 which is incompatible.\n",
            "gradio 5.50.0 requires pandas<3.0,>=1.0, but you have pandas 3.0.0 which is incompatible.\n",
            "bqplot 0.12.45 requires pandas<3.0.0,>=1.0.0, but you have pandas 3.0.0 which is incompatible.\n",
            "db-dtypes 1.5.0 requires pandas<3.0.0,>=1.5.3, but you have pandas 3.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed cartopy-0.25.0 cfgrib-0.9.15.1 eccodes-2.45.0 eccodeslib-2.45.1.9 eckitlib-1.32.4.9 fckitlib-0.14.1.9 findlibs-0.1.2 flexcache-0.3 flexparser-0.4 herbie-data-2026.1.1 metpy-1.7.1 pandas-3.0.0 pint-0.25.2 scikit-learn-1.8.0\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "libgeos-dev is already the newest version (3.12.1-1~jammy0).\n",
            "libgeos-dev set to manually installed.\n",
            "The following additional packages will be installed:\n",
            "  libeccodes-data libeccodes0\n",
            "The following NEW packages will be installed:\n",
            "  libeccodes-data libeccodes-dev libeccodes0\n",
            "0 upgraded, 3 newly installed, 0 to remove and 2 not upgraded.\n",
            "Need to get 3,076 kB of archives.\n",
            "After this operation, 60.8 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libeccodes-data all 2.24.2-1 [1,592 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libeccodes0 amd64 2.24.2-1 [614 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libeccodes-dev amd64 2.24.2-1 [870 kB]\n",
            "Fetched 3,076 kB in 1s (2,260 kB/s)\n",
            "Selecting previously unselected package libeccodes-data.\n",
            "(Reading database ... 117540 files and directories currently installed.)\n",
            "Preparing to unpack .../libeccodes-data_2.24.2-1_all.deb ...\n",
            "Unpacking libeccodes-data (2.24.2-1) ...\n",
            "Selecting previously unselected package libeccodes0:amd64.\n",
            "Preparing to unpack .../libeccodes0_2.24.2-1_amd64.deb ...\n",
            "Unpacking libeccodes0:amd64 (2.24.2-1) ...\n",
            "Selecting previously unselected package libeccodes-dev:amd64.\n",
            "Preparing to unpack .../libeccodes-dev_2.24.2-1_amd64.deb ...\n",
            "Unpacking libeccodes-dev:amd64 (2.24.2-1) ...\n",
            "Setting up libeccodes-data (2.24.2-1) ...\n",
            "Setting up libeccodes0:amd64 (2.24.2-1) ...\n",
            "Setting up libeccodes-dev:amd64 (2.24.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.11) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            " â•­â”€\u001b[48;2;255;255;255m\u001b[38;2;136;33;27mâ–Œ\u001b[0m\u001b[38;2;12;53;118m\u001b[48;2;240;234;210mâ–Œ\u001b[38;2;0;0;0m\u001b[1mHerbie\u001b[0mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
            " â”‚ INFO: Created a default config file.                 â”‚\n",
            " â”‚ You may view/edit Herbie's configuration here:       â”‚\n",
            " â”‚ \u001b[38;2;255;153;0m         /root/.config/herbie/config.toml         \u001b[0m   â”‚\n",
            " â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n",
            "\n",
            "âœ… Herbie 2026.1.1 with extras is ready.\n"
          ]
        }
      ],
      "source": [
        "# 1. Install Herbie with the full geospatial \"extras\"\n",
        "!pip install \"herbie-data[extras]\" cfgrib\n",
        "\n",
        "# 2. Install the necessary system drivers for GRIB2 files and mapping\n",
        "!apt-get install -y libeccodes-dev libgeos-dev\n",
        "\n",
        "# 3. Verify the installation\n",
        "import herbie\n",
        "print(f\"âœ… Herbie {herbie.__version__} with extras is ready.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpXIS5dnWPRH",
        "outputId": "f501dc5b-afd4-4ddd-e2f3-b2ea250e6bd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“¡ UNIFIED PRECISION SCAN: 2026-02-07\n",
            "CITY  | NWS    | ECMWF   | NBM     | GFS     | MEAN\n",
            "-----------------------------------------------------------------\n",
            "LAX   |  72.0Â°F |  66.9Â°F |  69.5Â°F |  69.5Â°F |  68.6Â°F\n",
            "LAS   |  74.0Â°F |  73.1Â°F |  73.5Â°F |  73.5Â°F |  73.4Â°F\n",
            "PHX   |  82.0Â°F |  80.6Â°F |  82.1Â°F |  82.1Â°F |  81.6Â°F\n",
            "SFO   |  62.0Â°F |  63.0Â°F |  63.5Â°F |  63.5Â°F |  63.3Â°F\n",
            "SEA   |  53.0Â°F |  50.7Â°F |  49.8Â°F |  49.8Â°F |  50.1Â°F\n",
            "AUS   |  81.0Â°F |  79.0Â°F |  82.2Â°F |  82.2Â°F |  81.1Â°F\n",
            "DEN   |  65.0Â°F |  61.8Â°F |  62.4Â°F |  62.4Â°F |  62.2Â°F\n",
            "NYC   |  22.0Â°F |  25.2Â°F |  25.3Â°F |  25.3Â°F |  25.3Â°F\n",
            "PHI   |  15.0Â°F |  27.7Â°F |  28.4Â°F |  28.4Â°F |  28.2Â°F\n",
            "DCA   |  21.0Â°F |  30.6Â°F |  30.8Â°F |  30.8Â°F |  30.7Â°F\n",
            "CHI   |  25.0Â°F |  21.7Â°F |  24.2Â°F |  24.2Â°F |  23.4Â°F\n",
            "MSP   |  26.0Â°F |  22.7Â°F |  28.0Â°F |  28.0Â°F |  26.2Â°F\n",
            "MSY   |  64.0Â°F |  61.7Â°F |  65.6Â°F |  65.6Â°F |  64.3Â°F\n",
            "ATL   |  47.0Â°F |  46.7Â°F |  50.5Â°F |  50.5Â°F |  49.2Â°F\n",
            "MIA   |  75.0Â°F |  75.2Â°F |  78.1Â°F |  78.1Â°F |  77.1Â°F\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from datetime import datetime\n",
        "\n",
        "# ==========================================\n",
        "# CONFIGURATION\n",
        "# ==========================================\n",
        "TARGET_DATE = \"2026-02-07\"\n",
        "\n",
        "CITIES = {\n",
        "    \"LAX\": {\"lat\": 33.94, \"lon\": -118.40, \"tz\": \"America/Los_Angeles\"},\n",
        "    \"LAS\": {\"lat\": 36.08, \"lon\": -115.15, \"tz\": \"America/Los_Angeles\"},\n",
        "    \"PHX\": {\"lat\": 33.43, \"lon\": -112.01, \"tz\": \"America/Phoenix\"},\n",
        "    \"SFO\": {\"lat\": 37.62, \"lon\": -122.37, \"tz\": \"America/Los_Angeles\"},\n",
        "    \"SEA\": {\"lat\": 47.45, \"lon\": -122.30, \"tz\": \"America/Los_Angeles\"},\n",
        "    \"AUS\": {\"lat\": 30.19, \"lon\": -97.66, \"tz\": \"America/Chicago\"},\n",
        "    \"DEN\": {\"lat\": 39.85, \"lon\": -104.67, \"tz\": \"America/Denver\"},\n",
        "    \"NYC\": {\"lat\": 40.78, \"lon\": -73.96, \"tz\": \"America/New_York\"},\n",
        "    \"PHI\": {\"lat\": 39.95, \"lon\": -75.16, \"tz\": \"America/New_York\"},\n",
        "    \"DCA\": {\"lat\": 38.85, \"lon\": -77.04,  \"tz\": \"America/New_York\"},\n",
        "    \"CHI\": {\"lat\": 41.78, \"lon\": -87.75, \"tz\": \"America/Chicago\"},\n",
        "    \"MSP\": {\"lat\": 44.88, \"lon\": -93.22, \"tz\": \"America/Chicago\"},\n",
        "    \"MSY\": {\"lat\": 29.99, \"lon\": -90.25,  \"tz\": \"America/Chicago\"},\n",
        "    \"ATL\": {\"lat\": 33.64, \"lon\": -84.43, \"tz\": \"America/New_York\"},\n",
        "    \"MIA\": {\"lat\": 25.79, \"lon\": -80.28, \"tz\": \"America/New_York\"}\n",
        "}\n",
        "\n",
        "def get_hourly_max(lat, lon, model, target_date, tz):\n",
        "    url = \"https://api.open-meteo.com/v1/forecast\"\n",
        "    params = {\n",
        "        \"latitude\": lat, \"longitude\": lon,\n",
        "        \"hourly\": \"temperature_2m\", \"models\": model,\n",
        "        \"temperature_unit\": \"fahrenheit\", \"timezone\": tz,\n",
        "        \"forecast_days\": 7\n",
        "    }\n",
        "    try:\n",
        "        res = requests.get(url, params=params).json()\n",
        "        # Find the specific key for the model requested\n",
        "        key = [k for k in res[\"hourly\"].keys() if \"temperature_2m\" in k][0]\n",
        "        temps = [t for time, t in zip(res[\"hourly\"][\"time\"], res[\"hourly\"][key])\n",
        "                 if time.startswith(target_date) and t is not None]\n",
        "        return max(temps) if temps else \"N/A\"\n",
        "    except: return \"N/A\"\n",
        "\n",
        "def get_nws_forecast(info, target_date):\n",
        "    \"\"\"Dynamically resolves the NWS grid using lat/lon info dictionary.\"\"\"\n",
        "    try:\n",
        "        point_url = f\"https://api.weather.gov/points/{info['lat']},{info['lon']}\"\n",
        "        point_res = requests.get(point_url, headers={'User-Agent': '(AlphaCommanderBot)'}, timeout=5).json()\n",
        "        forecast_url = point_res['properties']['forecast']\n",
        "\n",
        "        res = requests.get(forecast_url, headers={'User-Agent': '(AlphaCommanderBot)'}, timeout=5).json()\n",
        "        periods = res.get('properties', {}).get('periods', [])\n",
        "        for p in periods:\n",
        "            if target_date in p['startTime'] and \"night\" not in p['name'].lower():\n",
        "                return float(p['temperature'])\n",
        "        return \"N/A\"\n",
        "    except: return \"N/A\"\n",
        "\n",
        "# ==========================================\n",
        "# EXECUTION\n",
        "# ==========================================\n",
        "print(f\"ğŸ“¡ UNIFIED PRECISION SCAN: {TARGET_DATE}\")\n",
        "# Adjusted spacing for the new GFS column\n",
        "print(f\"{'CITY':<5} | {'NWS':<6} | {'ECMWF':<7} | {'NBM':<7} | {'GFS':<7} | {'MEAN'}\")\n",
        "print(\"-\" * 65)\n",
        "\n",
        "for city, info in CITIES.items():\n",
        "    # Fetching individual model highs\n",
        "    ec  = get_hourly_max(info['lat'], info['lon'], \"ecmwf_ifs\", TARGET_DATE, info['tz'])\n",
        "    nbm = get_hourly_max(info['lat'], info['lon'], \"best_match\", TARGET_DATE, info['tz'])\n",
        "    gfs = get_hourly_max(info['lat'], info['lon'], \"gfs_seamless\", TARGET_DATE, info['tz'])\n",
        "\n",
        "    # Pass the entire 'info' dictionary to the NWS function\n",
        "    nws = get_nws_forecast(info, TARGET_DATE)\n",
        "\n",
        "    # Calculate the Unified Mean (Average of EC, NBM, and GFS)\n",
        "    valid_temps = [t for t in [ec, nbm, gfs] if isinstance(t, (float, int))]\n",
        "    if valid_temps:\n",
        "        mean_val = sum(valid_temps) / len(valid_temps)\n",
        "        mean_str = f\"{mean_val:>5.1f}Â°F\"\n",
        "    else:\n",
        "        mean_str = \"  N/A  \"\n",
        "\n",
        "    # Helper function for pretty printing temperatures\n",
        "    f = lambda x: f\"{x:>5.1f}Â°F\" if isinstance(x, (float, int)) else \"  N/A  \"\n",
        "\n",
        "    print(f\"{city:<5} | {f(nws)} | {f(ec)} | {f(nbm)} | {f(gfs)} | {mean_str}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zk3rxCtdODbd",
        "outputId": "cf336530-3f89-4515-f7ab-b30ab6f00f09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ HERBIE: SYNCING MIDNIGHT-TO-MIDNIGHT FOR ALL 12 CITIES\n",
            "\n",
            "============================================================\n",
            "HERBIE : 2026-02-07 12:00Z range(0, 20) LOCAL MIDNIGHT-TO-MIDNIGHT HIGHS\n",
            "============================================================\n",
            "KLAX (LAX)         | HIGH:  69.88Â°F\n",
            "KLAS (Vegas)       | HIGH:  73.21Â°F\n",
            "KPHX (Phoenix)     | HIGH:  82.10Â°F\n",
            "KSFO (SFO)         | HIGH:  56.67Â°F\n",
            "KSEA (Seattle)     | HIGH:  51.08Â°F\n",
            "KAUS (Austin)      | HIGH:  81.68Â°F\n",
            "KDEN (Denver)      | HIGH:  63.13Â°F\n",
            "KNYC (NYC)         | HIGH:  18.22Â°F\n",
            "KPHL (Philly)      | HIGH:  24.29Â°F\n",
            "KDCA (Wash DC)     | HIGH:  20.81Â°F\n",
            "KMDW (Chicago)     | HIGH:  24.88Â°F\n",
            "KMSP (Minn)        | HIGH:  27.46Â°F\n",
            "KMSY (NOLA)        | HIGH:  65.93Â°F\n",
            "KATL (ATL)         | HIGH:  51.81Â°F\n",
            "KMIA (Miami)       | HIGH:  78.24Â°F\n"
          ]
        }
      ],
      "source": [
        "from herbie import Herbie\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# ==========================================\n",
        "# MASTER CONFIG: ALL-CITY MIDNIGHT RUN\n",
        "# ==========================================\n",
        "MODEL_DATE = '2026-02-07'\n",
        "MODEL_RUN  = '12:00'\n",
        "TARGET_DAY = '2026-02-07'\n",
        "\n",
        "# 00:00 range (5,32 9pm pacific, day before) (29, 48)  0= 4pm pacific day before run, covers the start of EST midnight to the end of PST midnight\n",
        "# 6:00  range (0,26 next day) (23, 47 2 day out)  0=10pm pacific day of run, covers the start of EST midnight to the end of PST midnight\n",
        "# 12:00 range (0,20 20hour of day) (17, 44 9pm pacific day of run) 0= 4am pacific day of run, 17= 9pm pacific covers the start of EST midnight to the end of PST midnight\n",
        "# 18:00 range (0, 14) (14, 38 9pm pacific day of run, covers next day) 0=10am covers 14 hours day of, can only look at next day pacific day of covers the start of EST midnight to the end of PST midnight\n",
        "forecast_hours = range(0,20)\n",
        "\n",
        "stations = {\n",
        "    'KLAX (LAX)':     {'lat': 33.94, 'lon': -118.41, 'offset': -8},\n",
        "    'KLAS (Vegas)':    {'lat': 36.08, 'lon': -115.15, 'offset': -8},\n",
        "    'KPHX (Phoenix)':  {'lat': 33.43, 'lon': -112.01, 'offset': -7},\n",
        "    'KSFO (SFO)':      {'lat': 37.62, 'lon': -122.37, 'offset': -8},\n",
        "    'KSEA (Seattle)':  {'lat': 47.45, 'lon': -122.30, 'offset': -8},\n",
        "    'KAUS (Austin)':  {'lat': 30.19, 'lon': -97.66, 'offset': -6},\n",
        "    'KDEN (Denver)':  {'lat': 39.86, 'lon': -104.67, 'offset': -7},\n",
        "    'KNYC (NYC)':     {'lat': 40.78, 'lon': -73.97, 'offset': -5},\n",
        "    'KPHL (Philly)':  {'lat': 39.87, 'lon': -75.24, 'offset': -5},\n",
        "    'KDCA (Wash DC)':  {'lat': 38.85, 'lon': -77.04,  'offset': -5},\n",
        "    'KMDW (Chicago)': {'lat': 41.78, 'lon': -87.75, 'offset': -6},\n",
        "    'KMSP (Minn)':    {'lat': 44.88, 'lon': -93.22, 'offset': -6},\n",
        "    'KMSY (NOLA)':     {'lat': 29.99, 'lon': -90.25,  'offset': -6},\n",
        "    'KATL (ATL)':  {'lat': 33.64, 'lon': -84.43, 'offset': -5},\n",
        "    'KMIA (Miami)':   {'lat': 25.79, 'lon': -80.29, 'offset': -5},\n",
        "}\n",
        "\n",
        "print(f\"ğŸš€ HERBIE: SYNCING MIDNIGHT-TO-MIDNIGHT FOR ALL 12 CITIES\")\n",
        "all_data = {site: [] for site in stations}\n",
        "base_time = datetime.strptime(f\"{MODEL_DATE} {MODEL_RUN}\", \"%Y-%m-%d %H:%M\")\n",
        "try:\n",
        "    for fxx in forecast_hours:\n",
        "        # Calculate the actual UTC time for this forecast hour\n",
        "        current_utc_time = base_time + timedelta(hours=fxx)\n",
        "        print(f\"Analyzing {current_utc_time.strftime('%H:%M UTC')} (f{fxx:02})...\", end=\"\\r\")\n",
        "\n",
        "        H = Herbie(f\"{MODEL_DATE} {MODEL_RUN}\", model='hrrr', product='sfc', fxx=fxx, verbose=False)\n",
        "        ds = H.xarray('TMP:2 m')\n",
        "        lats, lons, temps = ds.latitude.values, ds.longitude.values, ds.t2m.values\n",
        "        if lons.max() > 180: lons = (lons + 180) % 360 - 180\n",
        "\n",
        "        for name, info in stations.items():\n",
        "            # Check if this UTC hour belongs to the city's TARGET_DAY\n",
        "            local_time = current_utc_time + timedelta(hours=info['offset'])\n",
        "            if local_time.strftime(\"%Y-%m-%d\") == TARGET_DAY:\n",
        "                # Find nearest grid point and extract temp\n",
        "                dist = (lats - info['lat'])**2 + (lons - info['lon'])**2\n",
        "                y, x = np.unravel_index(dist.argmin(), dist.shape)\n",
        "                temp_f = (temps[y, x] - 273.15) * 9/5 + 32\n",
        "                all_data[name].append(temp_f)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\"HERBIE : {MODEL_DATE} {MODEL_RUN}Z {forecast_hours} LOCAL MIDNIGHT-TO-MIDNIGHT HIGHS\")\n",
        "    print(\"=\"*60)\n",
        "    for name, temps_list in all_data.items():\n",
        "        if temps_list:\n",
        "            print(f\"{name:<18} | HIGH: {max(temps_list):>6.2f}Â°F\")\n",
        "        else:\n",
        "            print(f\"{name:<18} | âš ï¸ No data captured for local window\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nâŒ Operational Error: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OAc0lxxnHOQl",
        "outputId": "be3267f0-8259-423b-da5b-63a17ba8739b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "STATION: KLAX | OFFICE: LOX\n",
            "============================================================\n",
            "\n",
            "000\n",
            "FXUS66 KLOX 071128\n",
            "AFDLOX\n",
            "\n",
            "Area Forecast Discussion\n",
            "National Weather Service Los Angeles/Oxnard CA \n",
            "328 AM PST Sat Feb 7 2026\n",
            "\n",
            ".SYNOPSIS...07/258 AM.\n",
            "\n",
            "A warming trend will continue through Sunday, then a weak to \n",
            "moderate storm system will move into the region and bring cooler\n",
            "temperatures and a chance of precipitation between Tuesday night and\n",
            "Wednesday night. A warming trend is possible for late next week,\n",
            "then a cool, wet, and unsettled weather pattern looks to resume \n",
            "for next weekend into the third week of the month.\n",
            "\n",
            "&&\n",
            "\n",
            ".SHORT TERM (TDY-MON)...07/252 AM.\n",
            "\n",
            "The latest satellite imagery shows an upper-level trough of low \n",
            "pressure now sitting well south of the area, near 30N and 120W,\n",
            "off the Baja California coast. Offshore flow is starting to\n",
            "develop as an upper-level ridge builds into the San Francisco Bay\n",
            "area. A warming trend will develop across the region through the\n",
            "weekend. The latest pressure gradients indicate offshore flow\n",
            "starting to reestablish and will likely turn negative later this \n",
            "morning. Temperatures will be well above normal for this time of \n",
            "year with 70s to lower 80s common across the coast and valleys \n",
            "today. Additional warming will take place into Sunday as 500 mb \n",
            "heights climb to near 581 decameters across the Southland.\n",
            "\n",
            "The winds will be tricky due to the north-northeasterly direction\n",
            "and the lack of upper-level support. Being less of a \"true\" Santa\n",
            "Ana, the north-northeast direction is a little less typical, but \n",
            "there is a low chance tha... [Full text displayed above]\n",
            "\n",
            "============================================================\n",
            "STATION: KLAS | OFFICE: VEF\n",
            "============================================================\n",
            "through next Friday.\n",
            "\n",
            "Dry and mild weather will return over the weekend as high pressure \n",
            "builds over the area. A shortwave will roll through the Pacific \n",
            "Northwest on Monday, resulting in decreasing 500 mb heights aloft \n",
            "and bringing increased cloud cover to the region, which, in turn, \n",
            "will result in cooling temperatures. Beyond Monday, weather will \n",
            "become cooler and more unsettled across the region as a shortwave \n",
            "rolls through the Great Basin on Tuesday and Wednesday, bringing \n",
            "increased precipitation chances to the higher terrain of Inyo County \n",
            "and the southern Great Basin, breezy southwesterly winds to the \n",
            "Mojave Desert, and cooler temperatures to the whole region. \n",
            "Uncertainty continues to plague the mid-week precipitation forecast \n",
            "due to interensemble variability regarding how much moisture will be \n",
            "available to help fuel rain and snow showers. While this shortwave \n",
            "brings unsettled weather to the area on Tuesday and Wednesday, an \n",
            "upper-level trough will be digging along the West Coast before \n",
            "eventually moving inland late next week/weekend as our active\n",
            "pattern looks to continue.... [See full text for more]\n",
            "\n",
            "============================================================\n",
            "STATION: KPHX | OFFICE: PSR\n",
            "============================================================\n",
            "\n",
            "000\n",
            "FXUS65 KPSR 071710\n",
            "AFDPSR\n",
            "\n",
            "Area Forecast Discussion\n",
            "National Weather Service Phoenix AZ\n",
            "1010 AM MST Sat Feb 7 2026\n",
            "\n",
            ".UPDATE...\n",
            "Updated Aviation...\n",
            "\n",
            "&&\n",
            "\n",
            ".KEY MESSAGES...\n",
            "\n",
            "- Well above normal temperatures will continue into early next week \n",
            "with lower desert highs reaching the lower to middle eighties \n",
            "resulting in Minor Heat Risk across the lower deserts.\n",
            "\n",
            "- A pattern change next week will cool temperatures somewhat and may \n",
            "eventually bring rain chances into the following weekend.\n",
            "\n",
            "&&\n",
            "\n",
            ".SHORT TERM /TODAY THROUGH SUNDAY/... \n",
            "Ridging that has been over much of Western CONUS the past several \n",
            "days is now to our east into the Plains. Meanwhile the cut off low \n",
            "that developed earlier yesterday off the SW coast of CA continues to \n",
            "move southeastwards and is expected to continue down the Baja Coast \n",
            "through today and tomorrow. The cut off low will be too far south to \n",
            "really affect H5 heights over our region, resulting in afternoon \n",
            "high temperatures to remain 8F-14F degrees above normal as readings \n",
            "top out in the low to mid 80s across the lower deserts. Early \n",
            "morning low temperatures will also remain above normal with most \n",
            "areas bottoming out in the mid 40s to mid 50s, with the potential \n",
            "for near record warm lows to be achieved in Phoenix. With the very \n",
            "warm afternoons in place, it is encouraged to stay hydrated and take \n",
            "breaks as needed, especially for travelers who may not be accustomed \n",
            "to these temperatures at this time of year as minor HeatRisk is in \n",
            "effect for the... [Full text displayed above]\n",
            "\n",
            "============================================================\n",
            "STATION: KSFO | OFFICE: MTR\n",
            "============================================================\n",
            "\n",
            "000\n",
            "FXUS66 KMTR 071228\n",
            "AFDMTR\n",
            "\n",
            "Area Forecast Discussion\n",
            "National Weather Service San Francisco CA\n",
            "428 AM PST Sat Feb 7 2026\n",
            "\n",
            "...New AVIATION, MARINE...\n",
            "\n",
            ".KEY MESSAGES...\n",
            "Updated at 203 AM PST Sat Feb 7 2026\n",
            "\n",
            " - Hazardous beach conditions from swells through this evening \n",
            "   for Pacific Coast beaches.\n",
            "\n",
            " - 20 to 40% chance of rain in the North Bay Sunday night.\n",
            "\n",
            " - Better chances for rain middle of next week.\n",
            "\n",
            " - Additional rain is expected next weekend.\n",
            "\n",
            "&&\n",
            "\n",
            ".SHORT TERM...\n",
            "Issued at 205 AM PST Sat Feb 7 2026\n",
            "(Today and tonight)\n",
            "\n",
            "Current nighttime microphysics satellite imagery shows a large \n",
            "swath of high clouds slowly overspreading the Bay Area. \n",
            "Eventually, these high clouds will obscure the view of the low \n",
            "level stratus and fog that has developed over the waters and into \n",
            "the North and East Bays, and Salinas Valley. Many obs around and \n",
            "south of SFO/OAK are showing more of a low stratus layer with \n",
            "ceilings somewhere between 400 and 600 feet. Whereas looking at \n",
            "local webcams in the North Bay, it appears that any dense fog is \n",
            "isolated which prevents the need for an advisory at the moment. \n",
            "However, should conditions continue to deteriorate, it is possible\n",
            "that a Dense Fog advisory may be needed.\n",
            "\n",
            "Other than the increased cloud cover, conditions today should be \n",
            "fairly similar to yesterday. High temperatures in the upper \n",
            "50s/low 60s along the coast, mostly 60s inland, and the chance for\n",
            "low 70s in southern Monterey and San Benito counties. Skies \n",
            "should clear out once ag... [Full text displayed above]\n",
            "\n",
            "============================================================\n",
            "STATION: KSEA | OFFICE: SEW\n",
            "============================================================\n",
            "\n",
            "000\n",
            "FXUS66 KSEW 071020\n",
            "AFDSEW\n",
            "\n",
            "Area Forecast Discussion\n",
            "National Weather Service Seattle WA\n",
            "220 AM PST Sat Feb 7 2026\n",
            "\n",
            ".SYNOPSIS...A frontal system will move across Western Washington\n",
            "today for lowland rain and breezy winds. Another system will\n",
            "move into Oregon and southern Washington Sunday. Unsettled\n",
            "weather continues early next week with split flow. A brief\n",
            "period of drier weather Wednesday before additional systems late\n",
            "week.\n",
            "\n",
            "&&\n",
            "\n",
            ".SHORT TERM /TODAY THROUGH MONDAY/...Rain is just offshore as of\n",
            "200 AM early this morning as the next frontal system approaches.\n",
            "Rain will spread inland through the morning as the front moves\n",
            "across Western Washington today. Southerly winds will also be\n",
            "breezy into the afternoon. Wind gusts will be strongest along\n",
            "the coast, and from Whidbey Island northwards, where gusts of 25\n",
            "to 40 MPH are expected. Localized gusts to 45 MPH may also occur\n",
            "around Whidbey Island, Bellingham, and the north coast, but are\n",
            "not expected to be widespread. The bulk of the precipitation\n",
            "will push east of the lowlands by this evening, with lingering\n",
            "precipitation into the Cascades tonight. Another system will\n",
            "move into Oregon on Sunday, with the best chance of\n",
            "precipitation Sunday from Snohomish County southwards. Snow\n",
            "levels around 6000 feet today will drop towards 4000 to 4500\n",
            "feet by Sunday afternoon.\n",
            "\n",
            "Cool, unsettled conditions continue Sunday night and Monday \n",
            "with lingering, light precipitation. Snow levels fall to 3000 \n",
            "to 3500 feet during this period, resulti... [Full text displayed above]\n",
            "\n",
            "============================================================\n",
            "STATION: KAUS | OFFICE: EWX\n",
            "============================================================\n",
            "\n",
            "000\n",
            "FXUS64 KEWX 071129\n",
            "AFDEWX\n",
            "\n",
            "Area Forecast Discussion\n",
            "National Weather Service Austin/San Antonio TX\n",
            "529 AM CST Sat Feb 7 2026\n",
            "\n",
            "...New AVIATION...\n",
            "\n",
            ".KEY MESSAGES...\n",
            "\n",
            "- Well above average temperatures today through next week\n",
            "\n",
            "- Low to medium chances for rain Monday night-Tuesday and Friday\n",
            "\n",
            "&&\n",
            "\n",
            ".SHORT TERM...\n",
            "(Today through Sunday)\n",
            "Issued at 1208 AM CST Sat Feb 7 2026\n",
            "\n",
            "A weak surface boundary drifts into our Central Texas counties early \n",
            "this morning, then lifts back north this afternoon as southerly \n",
            "lower level flow takes hold due to surface low pressure emerging \n",
            "over the Central High Plains. No impacts are expected. A dry airmass \n",
            "lingers today with light winds most areas. However, speeds of 10+ \n",
            "mph across the Rio Grande and Edwards Plateau could create elevated \n",
            "fire weather conditions there this afternoon. An upper level ridge \n",
            "moves off to the east later today allowing for a moist southwesterly \n",
            "flow aloft while moisture slowly increases in the southerly lower \n",
            "level flow tonight into Sunday. Due to a lower level thermal ridge, \n",
            "well above average high temperatures continue, though highs Sunday \n",
            "will be slightly cooler than today due to cloud cover from the \n",
            "increased moisture. High temperature records for today, February 7th \n",
            "are safe, being in the upper 80s (I-35 sites) to lower 90s (Del\n",
            "Rio).\n",
            "\n",
            "&&\n",
            "\n",
            ".LONG TERM...\n",
            "(Sunday night through Friday)\n",
            "Issued at 1208 AM CST Sat Feb 7 2026\n",
            "\n",
            "An upper level trough currently over the Pacific just west of Baja \n",
            "California/souther... [Full text displayed above]\n",
            "\n",
            "============================================================\n",
            "STATION: KDEN | OFFICE: BOU\n",
            "============================================================\n",
            "\n",
            "000\n",
            "FXUS65 KBOU 071732\n",
            "AFDBOU\n",
            "\n",
            "Area Forecast Discussion\n",
            "National Weather Service Denver/Boulder CO\n",
            "1032 AM MST Sat Feb 7 2026\n",
            "\n",
            ".KEY MESSAGES...  \n",
            "\n",
            "- Dry with well above normal temperatures through Monday. \n",
            "\n",
            "- Elevated fire weather conditions this afternoon with a Red Flag\n",
            "  Warning in effect for the northeast plains of Colorado. \n",
            "\n",
            "- Pattern change next week will lead to multiple chances of\n",
            "  precipitation. Best chance for snow will be Wednesday into\n",
            "  Thursday in the mountains. \n",
            "\n",
            "&&\n",
            "\n",
            ".DISCUSSION /Through Friday/...\n",
            "Issued at 1141 PM MST Fri Feb 6 2026\n",
            "\n",
            "A weak shortwave trough passes through the ridge Saturday \n",
            "bringing an increase in high clouds. At the surface, a low over \n",
            "the Northern Plains will help tighten the pressure gradient with \n",
            "breezy/windy conditions for Saturday afternoon. The northeast \n",
            "plains are expected to be the windiest with gusts to 35 mph. These\n",
            "winds combined with relative humidities falling into the mid \n",
            "teens will result in Red Flag conditions over the northeast \n",
            "plains. The airmass doesn't change much today with temperatures \n",
            "well into the 60s across northeast Colorado with dry conditions \n",
            "continuing. \n",
            "\n",
            "Upper level ridging holds strong over Colorado Sunday and Monday.\n",
            "Dry and well above normal temperatures will continue. Stronger \n",
            "flow aloft and passing shortwave troughs will be to our north. \n",
            "This may cause winds to increase across far northern Colorado both\n",
            "Sunday and Monday. We finally see a cold front Monday \n",
            "evening/night from a shortwave trough... [Full text displayed above]\n",
            "\n",
            "============================================================\n",
            "STATION: KNYC | OFFICE: OKX\n",
            "============================================================\n",
            ".KEY MESSAGE 1...\n",
            "A strong vort max rotating westward around an upper negative\n",
            "trough was producing localized moderate to heavy snowfall \n",
            "across the Twin Forks, with snowfall reports as high as 7 inches\n",
            "across the far eastern Forks. Issued a short term Winter \n",
            "Weather Advisory through 500 PM.\n",
            "\n",
            "Very strong synoptic forcing from elongating NW to SE upper\n",
            "level negative trough will allow for a strong low to develop \n",
            "along the arctic front as it passes offshore. This will prolong \n",
            "snow across eastern portions of the region, especially farther \n",
            "east across Long Island and Southeastern CT. Heavy bands across\n",
            "the far eastern sections have already produced 5 to 6 inches as\n",
            "of 930 AM EST. See Public Information Statement (PNSOKX) for\n",
            "details on snowfall amounts.\n",
            "\n",
            "Dry conditions return this evening.\n",
            "\n",
            ".KEY MESSAGE 2...\n",
            "An Arctic front moved across the region this morning. A rapid \n",
            "increase in the pressure gradient will make for strengthening \n",
            "northwesterly flow. This will bring strong cold air advection \n",
            "and winds will be sustained near 25 to 35 mph for late this \n",
            "morning into this afternoon, with gusts as high as 45 to near 50\n",
            "mph this afternoon. Both the NAM and GFS still indicate the \n",
            "tropopause dropping a little below 400mb late this afternoon \n",
            "into early this evening. The NAM and GFS also indicate the \n",
            "1000-500 mb thickness decreases to just 5020 to 5060 meters.\n",
            "\n",
            "Top of the mixed layer from BUFKIT soundings from the RAP and \n",
            "GFS and somewhat from the NAM indicate top of mixed layer... [See full text for more]\n",
            "\n",
            "============================================================\n",
            "STATION: KPHL | OFFICE: PHI\n",
            "============================================================\n",
            "KEY MESSAGE 1...Dangerously cold and windy conditions will \n",
            "continue through Sunday morning as arctic high pressure builds \n",
            "into the region.\n",
            "\n",
            "Strong cold air advection develops behind the passage of the Arctic \n",
            "front, and temperatures will be crashing throughout the day\n",
            "today. 925mb- 850mb temperatures drop to -21C to -25C by late \n",
            "this afternoon. As a result, the max temperature for today\n",
            "already occurred during the pre-dawn hours, generally in the \n",
            "mid and upper 20s.\n",
            "\n",
            "Meanwhile, as arctic high pressure builds into the Great Lakes \n",
            "and Ohio Valley, low pressure over the western Atlantic will \n",
            "intensify and deepen. This results in a tight northwest pressure \n",
            "gradient over the Northeast and Mid-Atlantic. Winds increase to 20 \n",
            "to 35 mph with gusts up to 55 mph for most of the region and as high \n",
            "as 60 mph across Delmarva and the southern coast of New Jersey. The \n",
            "High Wind Warning and Wind Advisory remain in effect and will be in \n",
            "effect through midnight tonight.\n",
            "\n",
            "The strong winds and very cold daytime temperatures will result in \n",
            "wind chills below zero during the daytime hours today, ranging from \n",
            "around -5 to -15 degrees for most areas, and as low as -25 in the \n",
            "southern Poconos.\n",
            "\n",
            "Temperatures tonight will drop into the single digits by Sunday \n",
            "morning for most of the region and as low as -5 degrees in the \n",
            "southern Poconos and far northwest New Jersey. Wind chills will be \n",
            "as low as -30 degrees tonight in the southern Poconos and far \n",
            "northwest New Jersey. Minimum wind chill... [See full text for more]\n",
            "\n",
            "============================================================\n",
            "STATION: KDCA | OFFICE: LWX\n",
            "============================================================\n",
            "KEY MESSAGE 1...Dangerous, life-threatening cold conditions and\n",
            "damaging winds today.\n",
            "\n",
            "Confidence remains high for dangerous and life-threatening wind\n",
            "gusts and wind chills beginning early this morning and\n",
            "persisting through Sunday morning across the entire forecast\n",
            "area. Extreme Cold Warnings and Cold Weather Advisories are in \n",
            "place across the entire forecast area from tonight through \n",
            "Sunday morning.\n",
            "\n",
            "A strong low pressure system located offshore paired with Arctic\n",
            "high pressure moving in from Canada will yield a tight pressure\n",
            "gradient over the forecast area today into Sunday morning. \n",
            "Additionally, a low level jet intensifying over the forecast \n",
            "area Saturday morning allows stronger winds aloft to mix to the \n",
            "surface.\n",
            "\n",
            "Wind gusts will remain steady at 45-55 mph through this \n",
            "evening, with embedded gusts of 60-70 mph at times (higher gusts\n",
            "possible in the mountains). The High Wind Warnings and Wind \n",
            "Advisories remain in effect through this evening for the entire \n",
            "area.\n",
            "\n",
            "Wind chills will remain below zero through at least late Sunday\n",
            "morning. For the Alleghenies and Blue Ridge, wind chills bottom\n",
            "out between -20F to -30F, and possibly as cold as -40F at\n",
            "elevations above 4000 ft (such as Spruce Knob). Elsewhere, wind\n",
            "chills as low as -10F to -20F are expected.\n",
            "\n",
            "This is only the second Extreme Cold Warning (along with its \n",
            "predecessor - Wind Chill Warning) issued for the immediate DC \n",
            "and Baltimore metro areas (as far back as WWA records go). The\n",
            "current forecast has between... [See full text for more]\n",
            "\n",
            "============================================================\n",
            "STATION: KMDW | OFFICE: LOT\n",
            "============================================================\n",
            "Issued at 228 AM CST Sat Feb 7 2026\n",
            "\n",
            "After our mildest temperature in nearly 3 weeks on Friday, northerly\n",
            "winds early this morning continue to usher in colder air. Some\n",
            "lake effect flurries and a couple of isolated light snow \n",
            "showers are streaming inland into NW Indiana early this morning.\n",
            "Low inversion heights have been a big limiting factor in more\n",
            "robust lake effect snow and with inversion heights progged to\n",
            "continue to lower this morning, lake effect threat should end\n",
            "later this morning. Not expecting any accumulations.\n",
            "\n",
            "After a mainly sunny day, look for high and eventually mid level\n",
            "cloudiness to stream into the area toward, and especially after,\n",
            "sunset this evening. This cloudiness is in association with a\n",
            "low amplitude shortwave trough riding the northwest flow through\n",
            "the region tonight. Look for strengthening isentropic ascent\n",
            "tonight in advance of this wave as low and mid level flow back\n",
            "on the 280-295K theta surfaces. Low level air mass will be quite\n",
            "dry, so it will be a race to see if the virga can break through\n",
            "the dry low levels before strongest isentropic ascent shifts\n",
            "east of the area. There has been a westward trend in guidance in\n",
            "where the breach of the low level dry air will take place, with\n",
            "most guidance now suggesting a quick hit of snow over far\n",
            "northeast Illinois into northwest Indiana tonight. If snow does\n",
            "occur it looks to be brief, only lasting an hour or two, but \n",
            "could be enough to coat the ground. Confidence is still rather \n",
            "low, so held off on ... [See full text for more]\n",
            "\n",
            "============================================================\n",
            "STATION: KMSP | OFFICE: MPX\n",
            "============================================================\n",
            "Issued at 225 AM CST Sat Feb 7 2026\n",
            "\n",
            "Much cooler air has settled in this morning, with many locations \n",
            "north of I-94 dropping into the single digits already. Winds \n",
            "will increase out of the southeast ahead of a subtle wave on \n",
            "the backside of the exiting high pressure today. This will allow\n",
            "for some rebound in temperatures, though many locations will likely\n",
            "stay in the 20s, outside of southwest MN. There remains a low \n",
            "chance (less than 20 percent) for light precipitation across \n",
            "parts of western WI tonight, though forecast soundings would \n",
            "suggest that we do not saturate enough to support notable \n",
            "snowfall. We expect mostly flurries or a light dusting at best,\n",
            "if any precip even falls at all.\n",
            "\n",
            "Our warm-up continues into the start of next week with \n",
            "persistent southerly winds and greater heights building aloft \n",
            "through Monday. Surface temperatures will be a solid 10 to 15F \n",
            "above normal, with southwest MN having the best chance to exceed\n",
            "50F. Primarily zonal flow aloft will moderate conditions \n",
            "through the rest of the week, with several chances for weak \n",
            "waves to produce light precip. There is no signal for a major \n",
            "pattern change through mid-February, with above average \n",
            "temperatures and a thaw likely to continue through next weekend.\n",
            "This is highlighted by the CPC's latest 6-10 and 8-14 Day \n",
            "Outlooks.... [See full text for more]\n",
            "\n",
            "============================================================\n",
            "STATION: KMSY | OFFICE: LIX\n",
            "============================================================\n",
            "\n",
            "000\n",
            "FXUS64 KLIX 071106\n",
            "AFDLIX\n",
            "\n",
            "Area Forecast Discussion\n",
            "National Weather Service New Orleans LA\n",
            "506 AM CST Sat Feb 7 2026\n",
            "\n",
            "...New AVIATION...\n",
            "\n",
            ".KEY MESSAGES...\n",
            "Updated at 506 AM CST Sat Feb 7 2026\n",
            "\n",
            "- Small Craft Exercise Caution headlines are in effect for the\n",
            "  open Gulf waters through Saturday morning.\n",
            "\n",
            "- Dry weather is expected until at least Tuesday. A quick moving\n",
            "  shortwave may bring some light rain chances Tuesday or\n",
            "  Wednesday. An additional weather system could move across the\n",
            "  area next Friday or Saturday.\n",
            "\n",
            "- Above normal temperatures are expected for most of the period, \n",
            "  especially during the workweek next week.\n",
            "\n",
            "&&\n",
            "\n",
            ".SHORT TERM...\n",
            "(Tonight through Sunday night)\n",
            "Issued at 1109 PM CST Fri Feb 6 2026\n",
            "\n",
            "Upper trough axis has moved to near the New England Coast. Upper \n",
            "ridging was noted along the lee side of the Rockies. At the surface, \n",
            "high pressure was centered over the southern and western Gulf. A \n",
            "weak surface front/wind shift was near the Arkansas/Louisiana \n",
            "border region at early evening. Under mostly clear skies, late  \n",
            "evening temperatures ranged from the mid 40s to mid 50s.\n",
            "\n",
            "Upper ridging will gradually shift eastward into the Plains States \n",
            "and western Gulf on Sunday. The frontal system to our north will sag \n",
            "southward over the next 12 to 18 hours and be offshore by midday \n",
            "Saturday. A large area of high pressure at the surface will become \n",
            "centered over the eastern third of the country. Doubtful we'll even \n",
            "see much in the way of clouds, let alone prec... [Full text displayed above]\n",
            "\n",
            "============================================================\n",
            "STATION: KATL | OFFICE: FFC\n",
            "============================================================\n",
            "\n",
            "000\n",
            "FXUS62 KFFC 071729\n",
            "AFDFFC\n",
            "\n",
            "Area Forecast Discussion\n",
            "National Weather Service Peachtree City GA\n",
            "1229 PM EST Sat Feb 7 2026\n",
            "\n",
            "\n",
            "\n",
            "...New 18Z Aviation Discussion...\n",
            "\n",
            ".KEY MESSAGES...\n",
            "Updated at 1225 PM EST Sat Feb 7 2026\n",
            "\n",
            " - A Fire Danger Statement is in effect across much of north and\n",
            "   east central Georgia on Saturday due to very low relative\n",
            "   humidity and gusty winds.\n",
            "\n",
            " - A wind advisory is in effect for gusty northwest winds in the\n",
            "   northeast Georgia mountains until 1 PM EST. Bitterly cold wind\n",
            "   chills are possible in higher terrain tonight through Saturday\n",
            "   morning.\n",
            "\n",
            " - Warmer next week, with high temperatures running 6 to 14 degree\n",
            "   above seasonal averages.\n",
            "\n",
            "&&\n",
            "\n",
            ".SHORT TERM...\n",
            "(Today through Sunday)\n",
            "Issued at 306 AM EST Sat Feb 7 2026\n",
            "\n",
            "Cold and windy conditions this morning combined with dry air will \n",
            "start the day off with widespread windchill values blow freezing for \n",
            "the entire CWA. A few areas across north Ga could see an hour or two \n",
            "where those values sink into the low teens and approach single \n",
            "digits due to the influx of dry air and wind gusts around 30mph.\n",
            "\n",
            "As the sun comes up though, temps will gradually improve with highs \n",
            "in the upper 40s and low 50s with relative humidities falling into \n",
            "the 20% range for areas along and north of I-85. This mix will \n",
            "likely require a Fire Danger Statement and any outdoor burning \n",
            "should be done with extreme caution. For more information, see the \n",
            "Fire WEather Forecast. \n",
            "\n",
            "For areas south of I-85 temps will warm int... [Full text displayed above]\n",
            "\n",
            "============================================================\n",
            "STATION: KMIA | OFFICE: MFL\n",
            "============================================================\n",
            "\n",
            "000\n",
            "FXUS62 KMFL 071715\n",
            "AFDMFL\n",
            "\n",
            "Area Forecast Discussion\n",
            "National Weather Service Miami FL\n",
            "1215 PM EST Sat Feb 7 2026\n",
            "\n",
            "...New UPDATE, AVIATION...\n",
            "\n",
            ".KEY MESSAGES...\n",
            "Updated at 1202 PM EST Sat Feb 7 2026\n",
            "\n",
            "    - Hazardous marine conditions will continue across the\n",
            "      Atlantic waters this weekend.\n",
            "\n",
            "- Low relative humidity values could result in enhanced fire\n",
            "      behavior this afternoon.\n",
            "\n",
            "&&\n",
            "\n",
            ".UPDATE...\n",
            "Issued at 1202 PM EST Sat Feb 7 2026\n",
            "\n",
            "Northwesterly flow has continued at the surface this afternoon,\n",
            "resulting in the continued advection of a dry airmass across South\n",
            "Florida. Relative humidity values have begun to dip into the 30s\n",
            "and 40s as vertical mixing brings drier air down to the surface.\n",
            "While surface winds remain breezy, 850mb winds are more brisk\n",
            "which may transport smoke from any plumes further away. Outside of\n",
            "any enhanced fire weather concerns, it'll be a quiet comfortable\n",
            "day across South Florida as previously forecast.\n",
            "\n",
            "&&\n",
            "\n",
            ".SHORT TERM...\n",
            "(Today through Sunday)\n",
            "Issued at 1224 AM EST Sat Feb 7 2026\n",
            "\n",
            "The mid-level and upper-level ridge pattern begins to amplify more \n",
            "and also shift towards the Florida Peninsula throughout today. The \n",
            "surface high with this ridge will be centered in the western Gulf \n",
            "today, but will expand and start to shift eastward for the latter \n",
            "half of the weekend. With exceptionally dry air (PWATs under 0.5\") \n",
            "that is in the top 10% of driest days for this date and high \n",
            "pressure building over the area, an extremely calm and comfortable \n",
            "we... [Full text displayed above]\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import time\n",
        "\n",
        "# Mapping your requested stations to their parent NWS Forecast Offices (WFOs)\n",
        "station_to_wfo = {\n",
        "    'KLAX': 'LOX', # LAX\n",
        "    'KLAS': 'VEF', # Vegas\n",
        "    'KPHX': 'PSR', # PHX\n",
        "    'KSFO': 'MTR', # SFO\n",
        "    'KSEA': 'SEW', # Seattle\n",
        "    'KAUS': 'EWX', # Austin\n",
        "    'KDEN': 'BOU', # Denver\n",
        "    'KNYC': 'OKX', # NYC\n",
        "    'KPHL': 'PHI', # Phil\n",
        "    'KDCA': 'LWX', # DC\n",
        "    'KMDW': 'LOT', # Chi\n",
        "    'KMSP': 'MPX', # Minn\n",
        "    'KMSY': 'LIX', # NOLA\n",
        "    'KATL': 'FFC', # ATL\n",
        "    'KMIA': 'MFL'  # Miami\n",
        "}\n",
        "\n",
        "# The NWS API requires a User-Agent header (use your email or a generic string)\n",
        "headers = {'User-Agent': '(MyWeatherScript, contact: yourname@example.com)'}\n",
        "\n",
        "def get_latest_discussion(wfo_id, station_id):\n",
        "    try:\n",
        "        # Step 1: Get the list of latest Area Forecast Discussions for this office\n",
        "        list_url = f\"https://api.weather.gov/products/types/AFD/locations/{wfo_id}\"\n",
        "        response = requests.get(list_url, headers=headers)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        # Step 2: Get the ID of the most recent discussion\n",
        "        latest_product_url = response.json()['@graph'][0]['@id']\n",
        "\n",
        "        # Step 3: Fetch the actual text of that discussion\n",
        "        discussion_response = requests.get(latest_product_url, headers=headers)\n",
        "        discussion_response.raise_for_status()\n",
        "        full_text = discussion_response.json()['productText']\n",
        "\n",
        "        # Step 4: Display the header and a relevant snippet\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"STATION: {station_id} | OFFICE: {wfo_id}\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        # We search for \"SYNOPSIS\" or \"DISCUSSION\" to skip the technical header\n",
        "        if \".DISCUSSION...\" in full_text:\n",
        "            snippet = full_text.split(\".DISCUSSION...\")[1].split(\"&&\")[0].strip()\n",
        "            print(snippet[:1500] + \"... [See full text for more]\")\n",
        "        else:\n",
        "            print(full_text[:1500] + \"... [Full text displayed above]\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Could not retrieve data for {station_id}: {e}\")\n",
        "\n",
        "# Run the loop for all 12 stations\n",
        "for station, wfo in station_to_wfo.items():\n",
        "    get_latest_discussion(wfo, station)\n",
        "    time.sleep(1) # Small delay to be polite to the NWS servers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mD1L5K1HIqff",
        "outputId": "84ba6dfb-eb5b-45f8-a0d3-0b0e0085c82e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Global Weather Scan: Saturday, Feb 07, 2026 (26FEB07) ---\n",
            "CITY            | TICKER                    | TITLE           | YES ASK\n",
            "---------------------------------------------------------------------------\n",
            "Los Angeles     | KXHIGHLAX-26FEB07-T68     | Will the **high temp in LA** be <68Â° on Feb 7, 2026? | 2Â¢\n",
            "Los Angeles     | KXHIGHLAX-26FEB07-B68.5   | Will the **high temp in LA** be 68-69Â° on Feb 7, 2026? | 8Â¢\n",
            "Los Angeles     | KXHIGHLAX-26FEB07-B70.5   | Will the **high temp in LA** be 70-71Â° on Feb 7, 2026? | 35Â¢\n",
            "Los Angeles     | KXHIGHLAX-26FEB07-B72.5   | Will the **high temp in LA** be 72-73Â° on Feb 7, 2026? | 52Â¢\n",
            "Los Angeles     | KXHIGHLAX-26FEB07-B74.5   | Will the **high temp in LA** be 74-75Â° on Feb 7, 2026? | 13Â¢\n",
            "Los Angeles     | KXHIGHLAX-26FEB07-T75     | Will the **high temp in LA** be >75Â° on Feb 7, 2026? | 1Â¢\n",
            "Las Vegas       | KXHIGHTLV-26FEB07-T71     | Will the maximum temperature be  <71Â° on Feb 7, 2026? | 7Â¢\n",
            "Las Vegas       | KXHIGHTLV-26FEB07-B71.5   | Will the maximum temperature be  71-72Â° on Feb 7, 2026? | 31Â¢\n",
            "Las Vegas       | KXHIGHTLV-26FEB07-B73.5   | Will the maximum temperature be  73-74Â° on Feb 7, 2026? | 71Â¢\n",
            "Las Vegas       | KXHIGHTLV-26FEB07-B75.5   | Will the maximum temperature be  75-76Â° on Feb 7, 2026? | 7Â¢\n",
            "Las Vegas       | KXHIGHTLV-26FEB07-B77.5   | Will the maximum temperature be  77-78Â° on Feb 7, 2026? | 5Â¢\n",
            "Las Vegas       | KXHIGHTLV-26FEB07-T78     | Will the maximum temperature be  >78Â° on Feb 7, 2026? | 2Â¢\n",
            "Phoenix         | KXHIGHTPHX-26FEB07-T79    | Will the maximum temperature be  <79Â° on Feb 7, 2026? | 5Â¢\n",
            "Phoenix         | KXHIGHTPHX-26FEB07-B79.5  | Will the maximum temperature be  79-80Â° on Feb 7, 2026? | 7Â¢\n",
            "Phoenix         | KXHIGHTPHX-26FEB07-B81.5  | Will the maximum temperature be  81-82Â° on Feb 7, 2026? | 72Â¢\n",
            "Phoenix         | KXHIGHTPHX-26FEB07-B83.5  | Will the maximum temperature be  83-84Â° on Feb 7, 2026? | 40Â¢\n",
            "Phoenix         | KXHIGHTPHX-26FEB07-B85.5  | Will the maximum temperature be  85-86Â° on Feb 7, 2026? | 12Â¢\n",
            "Phoenix         | KXHIGHTPHX-26FEB07-T86    | Will the maximum temperature be  >86Â° on Feb 7, 2026? | 11Â¢\n",
            "San Francisco   | KXHIGHTSFO-26FEB07-T58    | Will the maximum temperature be  <58Â° on Feb 7, 2026? | 3Â¢\n",
            "San Francisco   | KXHIGHTSFO-26FEB07-B58.5  | Will the maximum temperature be  58-59Â° on Feb 7, 2026? | 19Â¢\n",
            "San Francisco   | KXHIGHTSFO-26FEB07-B60.5  | Will the maximum temperature be  60-61Â° on Feb 7, 2026? | 41Â¢\n",
            "San Francisco   | KXHIGHTSFO-26FEB07-B62.5  | Will the maximum temperature be  62-63Â° on Feb 7, 2026? | 50Â¢\n",
            "San Francisco   | KXHIGHTSFO-26FEB07-B64.5  | Will the maximum temperature be  64-65Â° on Feb 7, 2026? | 19Â¢\n",
            "San Francisco   | KXHIGHTSFO-26FEB07-T65    | Will the maximum temperature be  >65Â° on Feb 7, 2026? | 4Â¢\n",
            "Seattle         | KXHIGHTSEA-26FEB07-T50    | Will the maximum temperature be  <50Â° on Feb 7, 2026? | 6Â¢\n",
            "Seattle         | KXHIGHTSEA-26FEB07-B50.5  | Will the maximum temperature be  50-51Â° on Feb 7, 2026? | 44Â¢\n",
            "Seattle         | KXHIGHTSEA-26FEB07-B52.5  | Will the maximum temperature be  52-53Â° on Feb 7, 2026? | 57Â¢\n",
            "Seattle         | KXHIGHTSEA-26FEB07-B54.5  | Will the maximum temperature be  54-55Â° on Feb 7, 2026? | 12Â¢\n",
            "Seattle         | KXHIGHTSEA-26FEB07-B56.5  | Will the maximum temperature be  56-57Â° on Feb 7, 2026? | 5Â¢\n",
            "Seattle         | KXHIGHTSEA-26FEB07-T57    | Will the maximum temperature be  >57Â° on Feb 7, 2026? | 1Â¢\n",
            "Austin          | KXHIGHAUS-26FEB07-T77     | Will the **high temp in Austin** be <77Â° on Feb 7, 2026? | 2Â¢\n",
            "Austin          | KXHIGHAUS-26FEB07-B77.5   | Will the **high temp in Austin** be 77-78Â° on Feb 7, 2026? | 2Â¢\n",
            "Austin          | KXHIGHAUS-26FEB07-B79.5   | Will the **high temp in Austin** be 79-80Â° on Feb 7, 2026? | 19Â¢\n",
            "Austin          | KXHIGHAUS-26FEB07-B81.5   | Will the **high temp in Austin** be 81-82Â° on Feb 7, 2026? | 40Â¢\n",
            "Austin          | KXHIGHAUS-26FEB07-B83.5   | Will the **high temp in Austin** be 83-84Â° on Feb 7, 2026? | 49Â¢\n",
            "Austin          | KXHIGHAUS-26FEB07-T84     | Will the **high temp in Austin** be >84Â° on Feb 7, 2026? | 13Â¢\n",
            "Denver          | KXHIGHDEN-26FEB07-T61     | Will the **high temp in Denver** be <61Â° on Feb 7, 2026? | 1Â¢\n",
            "Denver          | KXHIGHDEN-26FEB07-B61.5   | Will the **high temp in Denver** be 61-62Â° on Feb 7, 2026? | 4Â¢\n",
            "Denver          | KXHIGHDEN-26FEB07-B63.5   | Will the **high temp in Denver** be 63-64Â° on Feb 7, 2026? | 35Â¢\n",
            "Denver          | KXHIGHDEN-26FEB07-B65.5   | Will the **high temp in Denver** be 65-66Â° on Feb 7, 2026? | 55Â¢\n",
            "Denver          | KXHIGHDEN-26FEB07-B67.5   | Will the **high temp in Denver** be 67-68Â° on Feb 7, 2026? | 16Â¢\n",
            "Denver          | KXHIGHDEN-26FEB07-T68     | Will the **high temp in Denver** be >68Â° on Feb 7, 2026? | 3Â¢\n",
            "New York        | KXHIGHNY-26FEB07-T24      | Will the **high temp in NYC** be <24Â° on Feb 7, 2026? | 1Â¢\n",
            "New York        | KXHIGHNY-26FEB07-B24.5    | Will the **high temp in NYC** be 24-25Â° on Feb 7, 2026? | 1Â¢\n",
            "New York        | KXHIGHNY-26FEB07-B26.5    | Will the **high temp in NYC** be 26-27Â° on Feb 7, 2026? | 100Â¢\n",
            "New York        | KXHIGHNY-26FEB07-B28.5    | Will the **high temp in NYC** be 28-29Â° on Feb 7, 2026? | 1Â¢\n",
            "New York        | KXHIGHNY-26FEB07-B30.5    | Will the **high temp in NYC** be 30-31Â° on Feb 7, 2026? | 1Â¢\n",
            "New York        | KXHIGHNY-26FEB07-T31      | Will the **high temp in NYC** be >31Â° on Feb 7, 2026? | 1Â¢\n",
            "Philadelphia    | KXHIGHPHIL-26FEB07-T25    | Will the **high temp in Philadelphia** be <25Â° on Feb 7, 2026? | 1Â¢\n",
            "Philadelphia    | KXHIGHPHIL-26FEB07-B25.5  | Will the **high temp in Philadelphia** be 25-26Â° on Feb 7, 2026? | 1Â¢\n",
            "Philadelphia    | KXHIGHPHIL-26FEB07-B27.5  | Will the **high temp in Philadelphia** be 27-28Â° on Feb 7, 2026? | 1Â¢\n",
            "Philadelphia    | KXHIGHPHIL-26FEB07-B29.5  | Will the **high temp in Philadelphia** be 29-30Â° on Feb 7, 2026? | 1Â¢\n",
            "Philadelphia    | KXHIGHPHIL-26FEB07-B31.5  | Will the **high temp in Philadelphia** be 31-32Â° on Feb 7, 2026? | 100Â¢\n",
            "Philadelphia    | KXHIGHPHIL-26FEB07-T32    | Will the **high temp in Philadelphia** be >32Â° on Feb 7, 2026? | 1Â¢\n",
            "Washington DC   | KXHIGHTDC-26FEB07-T28     | Will the maximum temperature be  <28Â° on Feb 7, 2026? | 1Â¢\n",
            "Washington DC   | KXHIGHTDC-26FEB07-B28.5   | Will the maximum temperature be  28-29Â° on Feb 7, 2026? | 1Â¢\n",
            "Washington DC   | KXHIGHTDC-26FEB07-B30.5   | Will the maximum temperature be  30-31Â° on Feb 7, 2026? | 1Â¢\n",
            "Washington DC   | KXHIGHTDC-26FEB07-B32.5   | Will the maximum temperature be  32-33Â° on Feb 7, 2026? | 1Â¢\n",
            "Washington DC   | KXHIGHTDC-26FEB07-B34.5   | Will the maximum temperature be  34-35Â° on Feb 7, 2026? | 100Â¢\n",
            "Washington DC   | KXHIGHTDC-26FEB07-T35     | Will the maximum temperature be  >35Â° on Feb 7, 2026? | 1Â¢\n",
            "Chicago         | KXHIGHCHI-26FEB07-T22     | Will the high temp in Chicago be <22Â° on Feb 7, 2026? | 1Â¢\n",
            "Chicago         | KXHIGHCHI-26FEB07-B22.5   | Will the high temp in Chicago be 22-23Â° on Feb 7, 2026? | 10Â¢\n",
            "Chicago         | KXHIGHCHI-26FEB07-B24.5   | Will the high temp in Chicago be 24-25Â° on Feb 7, 2026? | 57Â¢\n",
            "Chicago         | KXHIGHCHI-26FEB07-B26.5   | Will the high temp in Chicago be 26-27Â° on Feb 7, 2026? | 34Â¢\n",
            "Chicago         | KXHIGHCHI-26FEB07-B28.5   | Will the high temp in Chicago be 28-29Â° on Feb 7, 2026? | 6Â¢\n",
            "Chicago         | KXHIGHCHI-26FEB07-T29     | Will the high temp in Chicago be >29Â° on Feb 7, 2026? | 3Â¢\n",
            "Minneapolis     | KXHIGHTMIN-26FEB07-T22    | Will the maximum temperature be  <22Â° on Feb 7, 2026? | 1Â¢\n",
            "Minneapolis     | KXHIGHTMIN-26FEB07-B22.5  | Will the maximum temperature be  22-23Â° on Feb 7, 2026? | 1Â¢\n",
            "Minneapolis     | KXHIGHTMIN-26FEB07-B24.5  | Will the maximum temperature be  24-25Â° on Feb 7, 2026? | 18Â¢\n",
            "Minneapolis     | KXHIGHTMIN-26FEB07-B26.5  | Will the maximum temperature be  26-27Â° on Feb 7, 2026? | 86Â¢\n",
            "Minneapolis     | KXHIGHTMIN-26FEB07-B28.5  | Will the maximum temperature be  28-29Â° on Feb 7, 2026? | 45Â¢\n",
            "Minneapolis     | KXHIGHTMIN-26FEB07-T29    | Will the maximum temperature be  >29Â° on Feb 7, 2026? | 6Â¢\n",
            "New Orleans     | KXHIGHTNOLA-26FEB07-T59   | Will the maximum temperature be  <59Â° on Feb 7, 2026? | 1Â¢\n",
            "New Orleans     | KXHIGHTNOLA-26FEB07-B59.5 | Will the maximum temperature be  59-60Â° on Feb 7, 2026? | 1Â¢\n",
            "New Orleans     | KXHIGHTNOLA-26FEB07-B61.5 | Will the maximum temperature be  61-62Â° on Feb 7, 2026? | 1Â¢\n",
            "New Orleans     | KXHIGHTNOLA-26FEB07-B63.5 | Will the maximum temperature be  63-64Â° on Feb 7, 2026? | 24Â¢\n",
            "New Orleans     | KXHIGHTNOLA-26FEB07-B65.5 | Will the maximum temperature be  65-66Â° on Feb 7, 2026? | 72Â¢\n",
            "New Orleans     | KXHIGHTNOLA-26FEB07-T66   | Will the maximum temperature be  >66Â° on Feb 7, 2026? | 13Â¢\n",
            "Atlanta         | KXHIGHTATL-26FEB07-T44    | Will the maximum temperature be  <44Â° on Feb 7, 2026? | 1Â¢\n",
            "Atlanta         | KXHIGHTATL-26FEB07-B44.5  | Will the maximum temperature be  44-45Â° on Feb 7, 2026? | 1Â¢\n",
            "Atlanta         | KXHIGHTATL-26FEB07-B46.5  | Will the maximum temperature be  46-47Â° on Feb 7, 2026? | 1Â¢\n",
            "Atlanta         | KXHIGHTATL-26FEB07-B48.5  | Will the maximum temperature be  48-49Â° on Feb 7, 2026? | 1Â¢\n",
            "Atlanta         | KXHIGHTATL-26FEB07-B50.5  | Will the maximum temperature be  50-51Â° on Feb 7, 2026? | 59Â¢\n",
            "Atlanta         | KXHIGHTATL-26FEB07-T51    | Will the maximum temperature be  >51Â° on Feb 7, 2026? | 95Â¢\n",
            "Miami           | KXHIGHMIA-26FEB07-T72     | Will the **high temp in Miami** be <72Â° on Feb 7, 2026? | 1Â¢\n",
            "Miami           | KXHIGHMIA-26FEB07-B72.5   | Will the **high temp in Miami** be 72-73Â° on Feb 7, 2026? | 1Â¢\n",
            "Miami           | KXHIGHMIA-26FEB07-B74.5   | Will the **high temp in Miami** be 74-75Â° on Feb 7, 2026? | 1Â¢\n",
            "Miami           | KXHIGHMIA-26FEB07-B76.5   | Will the **high temp in Miami** be 76-77Â° on Feb 7, 2026? | 25Â¢\n",
            "Miami           | KXHIGHMIA-26FEB07-B78.5   | Will the **high temp in Miami** be 78-79Â° on Feb 7, 2026? | 79Â¢\n",
            "Miami           | KXHIGHMIA-26FEB07-T79     | Will the **high temp in Miami** be >79Â° on Feb 7, 2026? | 3Â¢\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# ==========================================\n",
        "# CONFIGURATION: PICK YOUR DATE HERE\n",
        "# Format: YYYY-MM-DD\n",
        "# ==========================================\n",
        "TARGET_DATE = \"2026-02-07\"\n",
        "# ==========================================\n",
        "\n",
        "# Map of cities to their official Kalshi Series prefixes\n",
        "CITY_PREFIXES = {\n",
        "    \"Los Angeles\": \"KXHIGHLAX\",\n",
        "    \"Las Vegas\": \"KXHIGHTLV\",\n",
        "    \"Phoenix\": \"KXHIGHTPHX\",\n",
        "    \"San Francisco\": \"KXHIGHTSFO\",\n",
        "    \"Seattle\": \"KXHIGHTSEA\",\n",
        "    \"Austin\": \"KXHIGHAUS\",\n",
        "    \"Denver\": \"KXHIGHDEN\",\n",
        "    \"New York\": \"KXHIGHNY\",\n",
        "    \"Philadelphia\": \"KXHIGHPHIL\",\n",
        "    \"Washington DC\": \"KXHIGHTDC\",\n",
        "    \"Chicago\": \"KXHIGHCHI\",\n",
        "    \"Minneapolis\": \"KXHIGHTMIN\",\n",
        "    \"New Orleans\": \"KXHIGHTNOLA\",\n",
        "    \"Atlanta\": \"KXHIGHTATL\",\n",
        "    \"Miami\" : \"KXHIGHMIA\"\n",
        "}\n",
        "\n",
        "def scan_all_markets(date_input):\n",
        "    # --- DATE PICKER LOGIC ---\n",
        "    # 1. Convert string to datetime object\n",
        "    target_dt = datetime.strptime(date_input, \"%Y-%m-%d\")\n",
        "\n",
        "    # 2. Format to Kalshi style: Year(26) + Month(JAN) + Day(02)\n",
        "    date_str = target_dt.strftime(\"%y%b%d\").upper()\n",
        "    human_label = target_dt.strftime(\"%A, %b %d, %Y\")\n",
        "\n",
        "    print(f\"\\n--- Global Weather Scan: {human_label} ({date_str}) ---\")\n",
        "    print(f\"{'CITY':<15} | {'TICKER':<25} | {'TITLE':<15} | {'YES ASK'}\")\n",
        "    print(\"-\" * 75)\n",
        "\n",
        "    for city_name, prefix in CITY_PREFIXES.items():\n",
        "        event_ticker = f\"{prefix}-{date_str}\"\n",
        "\n",
        "        try:\n",
        "            # Fetch the event to see all nested markets\n",
        "            event = client.get_event(event_ticker)\n",
        "            for market in event.markets:\n",
        "                # Fetch live market data for prices\n",
        "                m_data = client.get_market(market.ticker).market\n",
        "                print(f\"{city_name:<15} | {market.ticker:<25} | {market.title:<15} | {m_data.yes_ask}Â¢\")\n",
        "        except Exception:\n",
        "            # Triggered if the market for that specific date isn't open yet\n",
        "            print(f\"{city_name:<15} | {event_ticker:<25} | MARKET NOT OPEN | N/A\")\n",
        "\n",
        "# Execute the scan for your chosen date\n",
        "scan_all_markets(TARGET_DATE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IC-A6eU_-rBQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "675f8496-1e35-4001-c3a5-64c436a52e56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Processing all 12 cities into CSV ---\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'min_ts' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-74739602.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;34m\"series_ticker\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;34m\"status\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"settled\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0;34m\"min_close_ts\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmin_ts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0;34m\"limit\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     }\n",
            "\u001b[0;31mNameError\u001b[0m: name 'min_ts' is not defined"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "\n",
        "# 1. This is the COMPLETE, verified dictionary.\n",
        "# Make sure NO curly braces or quotes are missing.\n",
        "city_prefixes = {\n",
        "    \"Los Angeles\": \"KXHIGHLAX\",\n",
        "    \"Las Vegas\": \"KXHIGHTLV\",\n",
        "    \"San Francisco\": \"KXHIGHTSFO\",\n",
        "    \"Seattle\": \"KXHIGHTSEA\",\n",
        "    \"Austin\": \"KXHIGHAUS\",\n",
        "    \"Denver\": \"KXHIGHDEN\",\n",
        "    \"New York\": \"KXHIGHNY\",\n",
        "    \"Philadelphia\": \"KXHIGHPHIL\",\n",
        "    \"Washington DC\": \"KXHIGHTDC\",\n",
        "    \"Chicago\": \"KXHIGHCHI\",\n",
        "    \"New Orleans\": \"KXHIGHTNOLA\",\n",
        "    \"Miami\": \"KXHIGHMIA\"\n",
        "}\n",
        "\n",
        "settled_data = []\n",
        "print(f\"--- Processing all 12 cities into CSV ---\")\n",
        "\n",
        "# Ensure min_ts is defined correctly from your Jan 2nd cell\n",
        "for city, prefix in city_prefixes.items():\n",
        "    url = \"https://api.elections.kalshi.com/trade-api/v2/markets\"\n",
        "    params = {\n",
        "        \"series_ticker\": prefix,\n",
        "        \"status\": \"settled\",\n",
        "        \"min_close_ts\": min_ts,\n",
        "        \"limit\": 1000\n",
        "    }\n",
        "\n",
        "    response = requests.get(url, params=params)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        markets = response.json().get('markets', [])\n",
        "        print(f\"Processing {city}... ({len(markets)} markets found)\")\n",
        "\n",
        "        for m in markets:\n",
        "            # We only save the 'yes' (winning) results\n",
        "            if m.get('result') == 'yes':\n",
        "                ticker = m.get('ticker')\n",
        "                # Standard Kalshi Ticker split\n",
        "                date_str = ticker.split('-')[1]\n",
        "                # Extracting temp range from title\n",
        "                temp_text = m.get('title').split('be ')[1].split(' on')[0]\n",
        "\n",
        "                settled_data.append({\n",
        "                    \"City\": city,\n",
        "                    \"Date\": date_str,\n",
        "                    \"Winning_Bracket\": temp_text,\n",
        "                    \"Close_Time\": m.get('close_time'),\n",
        "                    \"Ticker\": ticker\n",
        "                })\n",
        "    else:\n",
        "        print(f\"Error for {city}: {response.status_code}\")\n",
        "\n",
        "# 3. Final Save\n",
        "df = pd.DataFrame(settled_data)\n",
        "df.to_csv('kalshi_historical_highs.csv', index=False)\n",
        "print(f\"\\nSuccess! Found {len(df)} total winning days across all 12 cities.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "XkWK9hNL1Y6W"
      },
      "outputs": [],
      "source": [
        "# 1. Install Herbie with the full geospatial \"extras\"\n",
        "!pip install \"herbie-data[extras]\" cfgrib\n",
        "\n",
        "# 2. Install the necessary system drivers for GRIB2 files and mapping\n",
        "!apt-get install -y libeccodes-dev libgeos-dev\n",
        "\n",
        "# 3. Verify the installation\n",
        "import herbie\n",
        "print(f\"âœ… Herbie {herbie.__version__} with extras is ready.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ip-OgOcB8iZo"
      },
      "outputs": [],
      "source": [
        "!pip install requests cryptography pandas matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8BepyadeH9A"
      },
      "outputs": [],
      "source": [
        "!pip install kalshi-python-sync"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DSPaFdtxdttn"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    from kalshi_python_sync import Configuration, KalshiClient\n",
        "except ImportError:\n",
        "    print(\"Installing Kalshi SDK...\")\n",
        "    !pip install -q kalshi-python-sync\n",
        "    from kalshi_python_sync import Configuration, KalshiClient\n",
        "\n",
        "from google.colab import userdata\n",
        "\n",
        "# 1. Setup Configuration\n",
        "HOST = \"https://api.elections.kalshi.com/trade-api/v2\"\n",
        "config = Configuration(host=HOST)\n",
        "\n",
        "# 2. Extract Secrets\n",
        "config.api_key_id = userdata.get('KALSHI_KEY_ID')\n",
        "# Clean the private key to ensure it's a valid PEM format\n",
        "private_key = userdata.get('KALSHI_PRIVATE_KEY').replace('\\\\n', '\\n')\n",
        "config.private_key_pem = private_key\n",
        "\n",
        "# 3. Initialize\n",
        "client = KalshiClient(config)\n",
        "\n",
        "print(\"Client initialized successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0v8Tf0MbmwU-"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import time\n",
        "from google.colab import userdata\n",
        "\n",
        "# Configuration\n",
        "BASE_URL = \"https://trading-api.kalshi.com/trade-api/v2\"\n",
        "\n",
        "def get_historical_market_data(KXHIGHLAX, 26JAN01, 26JAN23):\n",
        "    \"\"\"\n",
        "    Fetches hourly price data for all markets within a temperature event.\n",
        "    \"\"\"\n",
        "    # 1. Get Event Details to find all tickers in the series\n",
        "    event_path = f\"/events/{event_ticker}\"\n",
        "    headers = get_kalshi_headers(\"GET\", event_path) # Using the RSA-PSS function from before\n",
        "\n",
        "    response = requests.get(BASE_URL + event_path, headers=headers)\n",
        "    if response.status_code != 200:\n",
        "        print(f\"Error fetching event: {response.text}\")\n",
        "        return None\n",
        "\n",
        "    markets = response.json().get('event', {}).get('markets', [])\n",
        "    all_frames = []\n",
        "\n",
        "    # 2. Loop through each bracket (market) in the event\n",
        "    for market in markets:\n",
        "        ticker = market['ticker']\n",
        "        # 'floor' and 'cap' define the temperature range for this bracket\n",
        "        floor = market.get('floor_ext', market.get('floor'))\n",
        "\n",
        "        # 3. Fetch Hourly OHLCV (interval=60 minutes)\n",
        "        ohlcv_path = f\"/markets/{ticker}/ohlcv?start_ts={start_ts}&end_ts={end_ts}&interval=60\"\n",
        "        ohlcv_headers = get_kalshi_headers(\"GET\", ohlcv_path)\n",
        "\n",
        "        r_ohlcv = requests.get(BASE_URL + ohlcv_path, headers=ohlcv_headers)\n",
        "        data = r_ohlcv.json().get('candlesticks', [])\n",
        "\n",
        "        if data:\n",
        "            df = pd.DataFrame(data)\n",
        "            df['ticker'] = ticker\n",
        "            df['temp_floor'] = floor\n",
        "            # Use 'close_ask' or 'close_bid' as a proxy for probability\n",
        "            # Kalshi prices are 0-100; we convert to 0-1.0\n",
        "            df['prob'] = df['yes_bid'] / 100\n",
        "            all_frames.append(df)\n",
        "\n",
        "        time.sleep(0.1) # Rate limit protection\n",
        "\n",
        "    if not all_frames:\n",
        "        return None\n",
        "\n",
        "    # 4. Combine and Pivot to reconstruct the distribution\n",
        "    full_df = pd.concat(all_frames)\n",
        "    full_df['start_period'] = pd.to_datetime(full_df['start_period'], unit='s')\n",
        "\n",
        "    # Pivot so columns are temperature floors and rows are hourly timestamps\n",
        "    dist_df = full_df.pivot(index='start_period', columns='temp_floor', values='prob')\n",
        "    return dist_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1hFIzxeWOCTy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AP8BuomRoe-P"
      },
      "source": [
        "# Implementation Checklist - Transaction Coordination Platform\n",
        "\n",
        "## Phase 1: Foundation (Week 1) âœ… Build First\n",
        "\n",
        "### 1.1 Database Schema Setup\n",
        "**Priority: CRITICAL - Everything depends on this**\n",
        "\n",
        "```sql\n",
        "-- Start here - create these tables first\n",
        "\n",
        "-- offices table\n",
        "CREATE TABLE offices (\n",
        "    office_id VARCHAR(50) PRIMARY KEY,\n",
        "    office_name VARCHAR(255) NOT NULL,\n",
        "    brokerage_name VARCHAR(255),\n",
        "    state VARCHAR(2) NOT NULL,\n",
        "    created_at TIMESTAMP DEFAULT NOW()\n",
        ");\n",
        "\n",
        "-- agents table\n",
        "CREATE TABLE agents (\n",
        "    agent_id VARCHAR(50) PRIMARY KEY,\n",
        "    office_id VARCHAR(50) REFERENCES offices(office_id),\n",
        "    name VARCHAR(255) NOT NULL,\n",
        "    email VARCHAR(255) UNIQUE NOT NULL,\n",
        "    phone VARCHAR(20),\n",
        "    created_at TIMESTAMP DEFAULT NOW()\n",
        ");\n",
        "\n",
        "-- transactions table\n",
        "CREATE TABLE transactions (\n",
        "    transaction_id VARCHAR(50) PRIMARY KEY,\n",
        "    office_id VARCHAR(50) REFERENCES offices(office_id),\n",
        "    agent_id VARCHAR(50) REFERENCES agents(agent_id),\n",
        "    property_address TEXT NOT NULL,\n",
        "    transaction_type VARCHAR(50) NOT NULL, -- 'purchase', 'listing', 'lease'\n",
        "    status VARCHAR(50) NOT NULL, -- 'active', 'in_escrow', 'closing', 'closed', 'cancelled'\n",
        "    stage VARCHAR(50), -- 'active', 'submitted_for_closing', 'closing', 'closed'\n",
        "    closing_date DATE,\n",
        "    created_at TIMESTAMP DEFAULT NOW(),\n",
        "    updated_at TIMESTAMP DEFAULT NOW(),\n",
        "    submitted_at TIMESTAMP,\n",
        "    closed_at TIMESTAMP,\n",
        "    \n",
        "    -- Computed fields (updated by triggers)\n",
        "    total_documents INT DEFAULT 0,\n",
        "    verified_documents INT DEFAULT 0,\n",
        "    pending_reviews INT DEFAULT 0,\n",
        "    incomplete_items INT DEFAULT 0\n",
        ");\n",
        "\n",
        "-- document_templates table (what docs each office requires)\n",
        "CREATE TABLE document_templates (\n",
        "    template_id SERIAL PRIMARY KEY,\n",
        "    office_id VARCHAR(50) REFERENCES offices(office_id),\n",
        "    state VARCHAR(2) NOT NULL,\n",
        "    transaction_type VARCHAR(50) NOT NULL,\n",
        "    document_code VARCHAR(50) NOT NULL, -- 'RPA', 'AD', 'TDS', etc.\n",
        "    document_name VARCHAR(255) NOT NULL,\n",
        "    folder_name VARCHAR(255) NOT NULL,\n",
        "    is_required BOOLEAN DEFAULT false,\n",
        "    description TEXT,\n",
        "    created_at TIMESTAMP DEFAULT NOW(),\n",
        "    \n",
        "    UNIQUE(office_id, transaction_type, document_code)\n",
        ");\n",
        "\n",
        "-- documents table\n",
        "CREATE TABLE documents (\n",
        "    document_id SERIAL PRIMARY KEY,\n",
        "    transaction_id VARCHAR(50) REFERENCES transactions(transaction_id),\n",
        "    template_id INT REFERENCES document_templates(template_id),\n",
        "    \n",
        "    document_code VARCHAR(50) NOT NULL,\n",
        "    document_name VARCHAR(255) NOT NULL,\n",
        "    folder_name VARCHAR(255) NOT NULL,\n",
        "    filename VARCHAR(255) NOT NULL,\n",
        "    file_path TEXT NOT NULL,\n",
        "    file_hash VARCHAR(64), -- SHA-256 for duplicate detection\n",
        "    \n",
        "    status VARCHAR(50) NOT NULL, -- 'pending', 'verified', 'rejected', 'incomplete'\n",
        "    \n",
        "    uploaded_by VARCHAR(50) REFERENCES agents(agent_id),\n",
        "    uploaded_at TIMESTAMP DEFAULT NOW(),\n",
        "    reviewed_by VARCHAR(50), -- agent_id or 'ai_auto_verified'\n",
        "    reviewed_at TIMESTAMP,\n",
        "    \n",
        "    ai_classification JSONB, -- Full Claude response\n",
        "    confidence_score DECIMAL(3,2),\n",
        "    extracted_data JSONB, -- Property address, parties, dates, etc.\n",
        "    \n",
        "    is_executed BOOLEAN DEFAULT false,\n",
        "    \n",
        "    created_at TIMESTAMP DEFAULT NOW()\n",
        ");\n",
        "\n",
        "-- Create indexes for performance\n",
        "CREATE INDEX idx_transactions_office ON transactions(office_id);\n",
        "CREATE INDEX idx_transactions_agent ON transactions(agent_id);\n",
        "CREATE INDEX idx_transactions_status ON transactions(status);\n",
        "CREATE INDEX idx_documents_transaction ON documents(transaction_id);\n",
        "CREATE INDEX idx_documents_status ON documents(status);\n",
        "CREATE INDEX idx_documents_hash ON documents(file_hash);\n",
        "```\n",
        "\n",
        "**Test it:**\n",
        "```python\n",
        "# Insert test data\n",
        "test_office = {\n",
        "    \"office_id\": \"test_office_001\",\n",
        "    \"office_name\": \"Best Sac Homes Group\",\n",
        "    \"state\": \"CA\"\n",
        "}\n",
        "\n",
        "test_agent = {\n",
        "    \"agent_id\": \"agent_001\",\n",
        "    \"office_id\": \"test_office_001\",\n",
        "    \"name\": \"Test Agent\",\n",
        "    \"email\": \"test@example.com\"\n",
        "}\n",
        "\n",
        "test_transaction = {\n",
        "    \"transaction_id\": \"2025-00001\",\n",
        "    \"office_id\": \"test_office_001\",\n",
        "    \"agent_id\": \"agent_001\",\n",
        "    \"property_address\": \"123 Main St, Sacramento CA 95814\",\n",
        "    \"transaction_type\": \"purchase\",\n",
        "    \"status\": \"active\"\n",
        "}\n",
        "```\n",
        "\n",
        "### 1.2 Environment Setup\n",
        "**Priority: CRITICAL**\n",
        "\n",
        "```python\n",
        "# requirements.txt\n",
        "fastapi==0.104.1\n",
        "uvicorn==0.24.0\n",
        "anthropic==0.7.7\n",
        "python-multipart==0.0.6\n",
        "sqlalchemy==2.0.23\n",
        "psycopg2-binary==2.9.9\n",
        "asyncpg==0.29.0\n",
        "PyPDF2==3.0.1\n",
        "python-docx==1.1.0\n",
        "Pillow==10.1.0\n",
        "pytesseract==0.3.10\n",
        "boto3==1.33.6\n",
        "python-dotenv==1.0.0\n",
        "pydantic==2.5.0\n",
        "redis==5.0.1\n",
        "\n",
        "# .env file\n",
        "ANTHROPIC_API_KEY=your_key_here\n",
        "DATABASE_URL=postgresql://user:pass@localhost/transactiondb\n",
        "AWS_ACCESS_KEY_ID=your_key\n",
        "AWS_SECRET_ACCESS_KEY=your_secret\n",
        "S3_BUCKET_NAME=transaction-docs-bucket\n",
        "```\n",
        "\n",
        "**Install:**\n",
        "```bash\n",
        "pip install -r requirements.txt\n",
        "```\n",
        "\n",
        "### 1.3 Basic FastAPI Setup\n",
        "**Priority: HIGH**\n",
        "\n",
        "```python\n",
        "# main.py - Start with this bare bones version\n",
        "\n",
        "from fastapi import FastAPI, UploadFile, File, Form, HTTPException\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from typing import List\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "app = FastAPI(title=\"Transaction Coordinator Platform\")\n",
        "\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "@app.get(\"/\")\n",
        "async def root():\n",
        "    return {\"message\": \"Transaction Coordinator API\", \"status\": \"running\"}\n",
        "\n",
        "@app.get(\"/health\")\n",
        "async def health_check():\n",
        "    return {\n",
        "        \"status\": \"healthy\",\n",
        "        \"database\": \"connected\",  # Add real check later\n",
        "        \"anthropic_api\": \"configured\" if os.getenv(\"ANTHROPIC_API_KEY\") else \"missing\"\n",
        "    }\n",
        "\n",
        "# Test it works\n",
        "if __name__ == \"__main__\":\n",
        "    import uvicorn\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
        "```\n",
        "\n",
        "**Test it:**\n",
        "```bash\n",
        "python main.py\n",
        "# Visit http://localhost:8000/docs\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Phase 2: Core Document Classification (Week 1-2) ğŸ¯ Critical Path\n",
        "\n",
        "### 2.1 Document Content Extractor\n",
        "**Priority: HIGH - Needed before classification**\n",
        "\n",
        "```python\n",
        "# utils/content_extractor.py\n",
        "\n",
        "import PyPDF2\n",
        "from docx import Document\n",
        "from PIL import Image\n",
        "import pytesseract\n",
        "from typing import Optional\n",
        "\n",
        "class ContentExtractor:\n",
        "    \"\"\"Extract text from various file types\"\"\"\n",
        "    \n",
        "    async def extract(self, file_path: str, filename: str) -> str:\n",
        "        \"\"\"Main extraction method\"\"\"\n",
        "        ext = filename.lower().split('.')[-1]\n",
        "        \n",
        "        if ext == 'pdf':\n",
        "            return await self.extract_pdf(file_path)\n",
        "        elif ext in ['doc', 'docx']:\n",
        "            return await self.extract_docx(file_path)\n",
        "        elif ext in ['jpg', 'jpeg', 'png']:\n",
        "            return await self.extract_image(file_path)\n",
        "        elif ext == 'txt':\n",
        "            return await self.extract_txt(file_path)\n",
        "        else:\n",
        "            return f\"[Unsupported file type: {ext}]\"\n",
        "    \n",
        "    async def extract_pdf(self, file_path: str) -> str:\n",
        "        \"\"\"Extract from PDF\"\"\"\n",
        "        text_parts = []\n",
        "        \n",
        "        with open(file_path, 'rb') as file:\n",
        "            pdf_reader = PyPDF2.PdfReader(file)\n",
        "            \n",
        "            for page_num, page in enumerate(pdf_reader.pages, 1):\n",
        "                text = page.extract_text()\n",
        "                if text.strip():\n",
        "                    text_parts.append(f\"--- PAGE {page_num} ---\\n{text}\")\n",
        "        \n",
        "        return '\\n\\n'.join(text_parts) if text_parts else \"[No text found in PDF]\"\n",
        "    \n",
        "    async def extract_docx(self, file_path: str) -> str:\n",
        "        \"\"\"Extract from Word doc\"\"\"\n",
        "        doc = Document(file_path)\n",
        "        paragraphs = [p.text for p in doc.paragraphs if p.text.strip()]\n",
        "        return '\\n\\n'.join(paragraphs)\n",
        "    \n",
        "    async def extract_image(self, file_path: str) -> str:\n",
        "        \"\"\"OCR from image\"\"\"\n",
        "        image = Image.open(file_path)\n",
        "        return pytesseract.image_to_string(image)\n",
        "    \n",
        "    async def extract_txt(self, file_path: str) -> str:\n",
        "        \"\"\"Read text file\"\"\"\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            return f.read()\n",
        "```\n",
        "\n",
        "**Test it:**\n",
        "```python\n",
        "# Test with sample PDF\n",
        "extractor = ContentExtractor()\n",
        "content = await extractor.extract('/tmp/test.pdf', 'test.pdf')\n",
        "print(f\"Extracted {len(content)} characters\")\n",
        "print(content[:500])  # First 500 chars\n",
        "```\n",
        "\n",
        "### 2.2 Claude Classifier (THE CORE ENGINE)\n",
        "**Priority: CRITICAL - This is your product's brain**\n",
        "\n",
        "```python\n",
        "# ai/document_classifier.py\n",
        "\n",
        "from anthropic import Anthropic\n",
        "import json\n",
        "from typing import Dict, List\n",
        "import hashlib\n",
        "\n",
        "class DocumentClassifier:\n",
        "    def __init__(self, api_key: str):\n",
        "        self.client = Anthropic(api_key=api_key)\n",
        "        self.model = \"claude-sonnet-4-20250514\"\n",
        "    \n",
        "    async def classify(\n",
        "        self,\n",
        "        content: str,\n",
        "        filename: str,\n",
        "        transaction: Dict,\n",
        "        templates: List[Dict]\n",
        "    ) -> Dict:\n",
        "        \"\"\"Main classification method\"\"\"\n",
        "        \n",
        "        prompt = self._build_prompt(content, filename, transaction, templates)\n",
        "        \n",
        "        try:\n",
        "            response = self.client.messages.create(\n",
        "                model=self.model,\n",
        "                max_tokens=4000,\n",
        "                temperature=0,\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "            )\n",
        "            \n",
        "            result_text = response.content[0].text\n",
        "            \n",
        "            # Clean JSON if wrapped in markdown\n",
        "            if result_text.startswith('```json'):\n",
        "                result_text = result_text[7:-3]\n",
        "            \n",
        "            classification = json.loads(result_text.strip())\n",
        "            return classification\n",
        "            \n",
        "        except Exception as e:\n",
        "            return {\n",
        "                \"error\": str(e),\n",
        "                \"document_identification\": {\n",
        "                    \"matched_template_code\": \"OTHER\",\n",
        "                    \"confidence\": 0.0\n",
        "                },\n",
        "                \"recommended_action\": {\"action\": \"needs_tc_review\"}\n",
        "            }\n",
        "    \n",
        "    def _build_prompt(self, content: str, filename: str,\n",
        "                     transaction: Dict, templates: List[Dict]) -> str:\n",
        "        \"\"\"Build classification prompt\"\"\"\n",
        "        \n",
        "        # Truncate content if too long\n",
        "        if len(content) > 8000:\n",
        "            content = content[:8000] + \"\\n[...truncated...]\"\n",
        "        \n",
        "        templates_str = \"\\n\".join([\n",
        "            f\"- {t['document_code']}: {t['document_name']} (Folder: {t['folder_name']})\"\n",
        "            for t in templates\n",
        "        ])\n",
        "        \n",
        "        return f\"\"\"Classify this real estate document.\n",
        "\n",
        "TRANSACTION:\n",
        "- Property: {transaction['property_address']}\n",
        "- Type: {transaction['transaction_type']}\n",
        "- State: {transaction['state']}\n",
        "\n",
        "EXPECTED DOCUMENTS:\n",
        "{templates_str}\n",
        "\n",
        "UPLOADED FILE: {filename}\n",
        "\n",
        "CONTENT:\n",
        "{content}\n",
        "\n",
        "Return JSON with:\n",
        "{{\n",
        "    \"document_identification\": {{\n",
        "        \"matched_template_code\": \"CODE or OTHER\",\n",
        "        \"document_name\": \"name\",\n",
        "        \"confidence\": 0.0-1.0,\n",
        "        \"reasoning\": \"why\"\n",
        "    }},\n",
        "    \"folder_routing\": {{\n",
        "        \"folder_name\": \"folder name\",\n",
        "        \"is_required\": true/false\n",
        "    }},\n",
        "    \"extracted_data\": {{\n",
        "        \"property_address\": \"address or null\",\n",
        "        \"parties\": {{\"buyers\": [], \"sellers\": []}},\n",
        "        \"dates\": {{\"closing\": \"date\"}},\n",
        "        \"signatures\": {{\"required\": [], \"present\": [], \"missing\": []}}\n",
        "    }},\n",
        "    \"validation_checks\": {{\n",
        "        \"address_matches_transaction\": true/false,\n",
        "        \"has_all_signatures\": true/false\n",
        "    }},\n",
        "    \"issues_found\": [\n",
        "        {{\"severity\": \"critical\", \"issue\": \"description\", \"blocks_approval\": true}}\n",
        "    ],\n",
        "    \"recommended_action\": {{\n",
        "        \"action\": \"auto_approve|needs_tc_review|needs_agent_correction\"\n",
        "    }}\n",
        "}}\"\"\"\n",
        "```\n",
        "\n",
        "**Test it thoroughly:**\n",
        "```python\n",
        "# Test classifier\n",
        "classifier = DocumentClassifier(api_key=os.getenv(\"ANTHROPIC_API_KEY\"))\n",
        "\n",
        "test_transaction = {\n",
        "    \"property_address\": \"123 Main St, Sacramento CA\",\n",
        "    \"transaction_type\": \"purchase\",\n",
        "    \"state\": \"CA\"\n",
        "}\n",
        "\n",
        "test_templates = [\n",
        "    {\n",
        "        \"document_code\": \"RPA\",\n",
        "        \"document_name\": \"Residential Purchase Agreement\",\n",
        "        \"folder_name\": \"Required Sales Docs\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# Extract content from test PDF\n",
        "content = await extractor.extract('/tmp/test_rpa.pdf', 'test_rpa.pdf')\n",
        "\n",
        "# Classify\n",
        "classification = await classifier.classify(\n",
        "    content=content,\n",
        "    filename='test_rpa.pdf',\n",
        "    transaction=test_transaction,\n",
        "    templates=test_templates\n",
        ")\n",
        "\n",
        "print(json.dumps(classification, indent=2))\n",
        "\n",
        "# Verify:\n",
        "# - Does it identify document correctly?\n",
        "# - Is confidence reasonable?\n",
        "# - Does it catch missing signatures?\n",
        "# - Does address validation work?\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Phase 3: Storage & Routing (Week 2) ğŸ“\n",
        "\n",
        "### 3.1 File Storage Handler\n",
        "**Priority: HIGH**\n",
        "\n",
        "```python\n",
        "# storage/file_handler.py\n",
        "\n",
        "import boto3\n",
        "import os\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "import shutil\n",
        "\n",
        "class FileStorage:\n",
        "    def __init__(self, storage_type: str = \"local\"):\n",
        "        self.storage_type = storage_type\n",
        "        \n",
        "        if storage_type == \"s3\":\n",
        "            self.s3 = boto3.client('s3')\n",
        "            self.bucket = os.getenv('S3_BUCKET_NAME')\n",
        "        elif storage_type == \"local\":\n",
        "            self.base_path = Path(\"/tmp/transaction-docs\")\n",
        "            self.base_path.mkdir(exist_ok=True)\n",
        "    \n",
        "    def generate_path(self, transaction_id: str, folder: str,\n",
        "                     doc_code: str, filename: str) -> str:\n",
        "        \"\"\"Generate organized path\"\"\"\n",
        "        \n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        clean_folder = folder.replace(\" \", \"_\")\n",
        "        \n",
        "        return f\"{transaction_id}/{clean_folder}/{doc_code}_{timestamp}_{filename}\"\n",
        "    \n",
        "    async def save(self, temp_path: str, final_path: str) -> str:\n",
        "        \"\"\"Save file to storage\"\"\"\n",
        "        \n",
        "        if self.storage_type == \"s3\":\n",
        "            with open(temp_path, 'rb') as f:\n",
        "                self.s3.upload_fileobj(f, self.bucket, final_path)\n",
        "            return f\"s3://{self.bucket}/{final_path}\"\n",
        "        \n",
        "        else:  # local\n",
        "            full_path = self.base_path / final_path\n",
        "            full_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "            shutil.copy2(temp_path, full_path)\n",
        "            return str(full_path)\n",
        "```\n",
        "\n",
        "### 3.2 Document Router\n",
        "**Priority: HIGH**\n",
        "\n",
        "```python\n",
        "# routing/document_router.py\n",
        "\n",
        "from storage.file_handler import FileStorage\n",
        "from datetime import datetime\n",
        "import hashlib\n",
        "\n",
        "class DocumentRouter:\n",
        "    def __init__(self, storage: FileStorage, db):\n",
        "        self.storage = storage\n",
        "        self.db = db\n",
        "    \n",
        "    async def route_document(\n",
        "        self,\n",
        "        temp_path: str,\n",
        "        filename: str,\n",
        "        classification: Dict,\n",
        "        transaction_id: str,\n",
        "        uploaded_by: str\n",
        "    ) -> Dict:\n",
        "        \"\"\"Route classified document to final location\"\"\"\n",
        "        \n",
        "        # Calculate file hash\n",
        "        file_hash = self._calc_hash(temp_path)\n",
        "        \n",
        "        # Check for duplicates\n",
        "        duplicate = await self._check_duplicate(transaction_id, file_hash)\n",
        "        if duplicate:\n",
        "            return {\n",
        "                \"status\": \"duplicate\",\n",
        "                \"existing_document_id\": duplicate[\"document_id\"]\n",
        "            }\n",
        "        \n",
        "        # Generate final path\n",
        "        final_path = self.storage.generate_path(\n",
        "            transaction_id=transaction_id,\n",
        "            folder=classification[\"folder_routing\"][\"folder_name\"],\n",
        "            doc_code=classification[\"document_identification\"][\"matched_template_code\"],\n",
        "            filename=filename\n",
        "        )\n",
        "        \n",
        "        # Store file\n",
        "        stored_path = await self.storage.save(temp_path, final_path)\n",
        "        \n",
        "        # Determine status\n",
        "        action = classification[\"recommended_action\"][\"action\"]\n",
        "        if action == \"auto_approve\":\n",
        "            status = \"verified\"\n",
        "        elif action == \"needs_agent_correction\":\n",
        "            status = \"rejected\"\n",
        "        else:\n",
        "            status = \"pending\"\n",
        "        \n",
        "        # Create DB record\n",
        "        document = await self.db.execute(\"\"\"\n",
        "            INSERT INTO documents (\n",
        "                transaction_id, document_code, document_name,\n",
        "                folder_name, filename, file_path, file_hash,\n",
        "                status, uploaded_by, uploaded_at,\n",
        "                ai_classification, confidence_score,\n",
        "                extracted_data, is_executed\n",
        "            ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14)\n",
        "            RETURNING document_id\n",
        "        \"\"\",\n",
        "            transaction_id,\n",
        "            classification[\"document_identification\"][\"matched_template_code\"],\n",
        "            classification[\"document_identification\"][\"document_name\"],\n",
        "            classification[\"folder_routing\"][\"folder_name\"],\n",
        "            filename,\n",
        "            stored_path,\n",
        "            file_hash,\n",
        "            status,\n",
        "            uploaded_by,\n",
        "            datetime.now(),\n",
        "            json.dumps(classification),\n",
        "            classification[\"document_identification\"][\"confidence\"],\n",
        "            json.dumps(classification.get(\"extracted_data\", {})),\n",
        "            classification.get(\"document_analysis\", {}).get(\"is_executed\", False)\n",
        "        )\n",
        "        \n",
        "        # Update transaction counts\n",
        "        await self._update_transaction_counts(transaction_id)\n",
        "        \n",
        "        return {\n",
        "            \"status\": \"success\",\n",
        "            \"document_id\": document[\"document_id\"],\n",
        "            \"document_status\": status,\n",
        "            \"stored_path\": stored_path\n",
        "        }\n",
        "    \n",
        "    def _calc_hash(self, file_path: str) -> str:\n",
        "        \"\"\"Calculate SHA-256 hash\"\"\"\n",
        "        sha256 = hashlib.sha256()\n",
        "        with open(file_path, 'rb') as f:\n",
        "            for block in iter(lambda: f.read(4096), b\"\"):\n",
        "                sha256.update(block)\n",
        "        return sha256.hexdigest()\n",
        "    \n",
        "    async def _check_duplicate(self, transaction_id: str, file_hash: str):\n",
        "        \"\"\"Check if file already uploaded\"\"\"\n",
        "        return await self.db.query_one(\"\"\"\n",
        "            SELECT document_id FROM documents\n",
        "            WHERE transaction_id = $1 AND file_hash = $2\n",
        "        \"\"\", transaction_id, file_hash)\n",
        "    \n",
        "    async def _update_transaction_counts(self, transaction_id: str):\n",
        "        \"\"\"Update document counts on transaction\"\"\"\n",
        "        await self.db.execute(\"\"\"\n",
        "            UPDATE transactions SET\n",
        "                total_documents = (\n",
        "                    SELECT COUNT(*) FROM documents\n",
        "                    WHERE transaction_id = $1\n",
        "                ),\n",
        "                verified_documents = (\n",
        "                    SELECT COUNT(*) FROM documents\n",
        "                    WHERE transaction_id = $1 AND status = 'verified'\n",
        "                ),\n",
        "                pending_reviews = (\n",
        "                    SELECT COUNT(*) FROM documents\n",
        "                    WHERE transaction_id = $1 AND status = 'pending'\n",
        "                ),\n",
        "                incomplete_items = (\n",
        "                    SELECT COUNT(*) FROM documents\n",
        "                    WHERE transaction_id = $1 AND status IN ('rejected', 'incomplete')\n",
        "                )\n",
        "            WHERE transaction_id = $1\n",
        "        \"\"\", transaction_id)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Phase 4: API Endpoints (Week 2-3) ğŸ”Œ\n",
        "\n",
        "### 4.1 Upload Endpoint\n",
        "**Priority: CRITICAL**\n",
        "\n",
        "```python\n",
        "# Add to main.py\n",
        "\n",
        "from utils.content_extractor import ContentExtractor\n",
        "from ai.document_classifier import DocumentClassifier\n",
        "from routing.document_router import DocumentRouter\n",
        "from storage.file_handler import FileStorage\n",
        "import tempfile\n",
        "\n",
        "# Initialize components\n",
        "extractor = ContentExtractor()\n",
        "classifier = DocumentClassifier(api_key=os.getenv(\"ANTHROPIC_API_KEY\"))\n",
        "storage = FileStorage(storage_type=\"local\")  # Change to \"s3\" for production\n",
        "router = DocumentRouter(storage=storage, db=db)\n",
        "\n",
        "@app.post(\"/api/agents/upload\")\n",
        "async def upload_documents(\n",
        "    transaction_id: str = Form(...),\n",
        "    agent_id: str = Form(...),\n",
        "    files: List[UploadFile] = File(...),\n",
        "    notes: str = Form(None)\n",
        "):\n",
        "    \"\"\"Agent uploads documents - main entry point\"\"\"\n",
        "    \n",
        "    # Get transaction context\n",
        "    transaction = await db.query_one(\"\"\"\n",
        "        SELECT t.*, o.state, a.name as agent_name\n",
        "        FROM transactions t\n",
        "        JOIN offices o ON t.office_id = o.office_id\n",
        "        JOIN agents a ON t.agent_id = a.agent_id\n",
        "        WHERE t.transaction_id = $1\n",
        "    \"\"\", transaction_id)\n",
        "    \n",
        "    if not transaction:\n",
        "        raise HTTPException(404, \"Transaction not found\")\n",
        "    \n",
        "    # Get expected documents\n",
        "    templates = await db.query(\"\"\"\n",
        "        SELECT * FROM document_templates\n",
        "        WHERE office_id = $1\n",
        "        AND transaction_type = $2\n",
        "    \"\"\", transaction[\"office_id\"], transaction[\"transaction_type\"])\n",
        "    \n",
        "    results = []\n",
        "    \n",
        "    for file in files:\n",
        "        try:\n",
        "            # Save to temp location\n",
        "            with tempfile.NamedTemporaryFile(delete=False, suffix=Path(file.filename).suffix) as tmp:\n",
        "                content = await file.read()\n",
        "                tmp.write(content)\n",
        "                temp_path = tmp.name\n",
        "            \n",
        "            # Extract content\n",
        "            extracted = await extractor.extract(temp_path, file.filename)\n",
        "            \n",
        "            # Classify\n",
        "            classification = await classifier.classify(\n",
        "                content=extracted,\n",
        "                filename=file.filename,\n",
        "                transaction=transaction,\n",
        "                templates=[{\n",
        "                    \"document_code\": t[\"document_code\"],\n",
        "                    \"document_name\": t[\"document_name\"],\n",
        "                    \"folder_name\": t[\"folder_name\"]\n",
        "                } for t in templates]\n",
        "            )\n",
        "            \n",
        "            # Route & store\n",
        "            routed = await router.route_document(\n",
        "                temp_path=temp_path,\n",
        "                filename=file.filename,\n",
        "                classification=classification,\n",
        "                transaction_id=transaction_id,\n",
        "                uploaded_by=agent_id\n",
        "            )\n",
        "            \n",
        "            # Clean up temp\n",
        "            os.unlink(temp_path)\n",
        "            \n",
        "            results.append({\n",
        "                \"filename\": file.filename,\n",
        "                \"status\": \"success\",\n",
        "                \"classified_as\": classification[\"document_identification\"][\"document_name\"],\n",
        "                \"folder\": classification[\"folder_routing\"][\"folder_name\"],\n",
        "                \"confidence\": classification[\"document_identification\"][\"confidence\"],\n",
        "                \"document_id\": routed[\"document_id\"],\n",
        "                \"document_status\": routed[\"document_status\"],\n",
        "                \"issues\": classification.get(\"issues_found\", [])\n",
        "            })\n",
        "            \n",
        "        except Exception as e:\n",
        "            results.append({\n",
        "                \"filename\": file.filename,\n",
        "                \"status\": \"error\",\n",
        "                \"error\": str(e)\n",
        "            })\n",
        "    \n",
        "    return {\n",
        "        \"transaction_id\": transaction_id,\n",
        "        \"total_uploaded\": len(files),\n",
        "        \"successful\": len([r for r in results if r[\"status\"] == \"success\"]),\n",
        "        \"results\": results\n",
        "    }\n",
        "```\n",
        "\n",
        "**Test it:**\n",
        "```bash\n",
        "curl -X POST \"http://localhost:8000/api/agents/upload\" \\\n",
        "  -F \"transaction_id=2025-00001\" \\\n",
        "  -F \"agent_id=agent_001\" \\\n",
        "  -F \"files=@test_document.pdf\"\n",
        "```\n",
        "\n",
        "### 4.2 Broker Dashboard Endpoint\n",
        "**Priority: HIGH**\n",
        "\n",
        "```python\n",
        "@app.get(\"/api/broker/dashboard\")\n",
        "async def broker_dashboard(office_id: str):\n",
        "    \"\"\"Main broker dashboard\"\"\"\n",
        "    \n",
        "    # Get active transactions with document counts\n",
        "    transactions = await db.query(\"\"\"\n",
        "        SELECT\n",
        "            t.*,\n",
        "            a.name as agent_name,\n",
        "            t.total_documents,\n",
        "            t.verified_documents,\n",
        "            t.pending_reviews,\n",
        "            t.incomplete_items\n",
        "        FROM transactions t\n",
        "        JOIN agents a ON t.agent_id = a.agent_id\n",
        "        WHERE t.office_id = $1\n",
        "        AND t.status IN ('active', 'in_escrow', 'closing')\n",
        "        ORDER BY t.closing_date ASC NULLS LAST\n",
        "    \"\"\", office_id)\n",
        "    \n",
        "    # Calculate stats\n",
        "    stats = {\n",
        "        \"total_active\": len(transactions),\n",
        "        \"closing_this_week\": len([\n",
        "            t for t in transactions\n",
        "            if t[\"closing_date\"] and\n",
        "            (t[\"closing_date\"] - datetime.now().date()).days <= 7\n",
        "        ]),\n",
        "        \"total_pending_reviews\": sum(t[\"pending_reviews\"] for t in transactions),\n",
        "        \"with_issues\": len([t for t in transactions if t[\"incomplete_items\"] > 0])\n",
        "    }\n",
        "    \n",
        "    return {\n",
        "        \"stats\": stats,\n",
        "        \"transactions\": transactions\n",
        "    }\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Phase 5: Testing & Refinement (Week 3) ğŸ§ª\n",
        "\n",
        "### 5.1 Create Test Data Script\n",
        "```python\n",
        "# scripts/seed_test_data.py\n",
        "\n",
        "async def seed_test_data():\n",
        "    \"\"\"Create test office, agents, transactions\"\"\"\n",
        "    \n",
        "    # Create office\n",
        "    await db.execute(\"\"\"\n",
        "        INSERT INTO offices (office_id, office_name, state)\n",
        "        VALUES ('test_office', 'Test Brokerage', 'CA')\n",
        "    \"\"\")\n",
        "    \n",
        "    # Create agent\n",
        "    await db.execute(\"\"\"\n",
        "        INSERT INTO agents (agent_id, office_id, name, email)\n",
        "        VALUES ('agent_001', 'test_office', 'Test Agent', 'test@example.com')\n",
        "    \"\"\")\n",
        "    \n",
        "    # Create transaction\n",
        "    await db.execute(\"\"\"\n",
        "        INSERT INTO transactions (\n",
        "            transaction_id, office_id, agent_id,\n",
        "            property_address, transaction_type, status\n",
        "        ) VALUES (\n",
        "            '2025-00001', 'test_office', 'agent_001',\n",
        "            '123 Main St, Sacramento CA 95814', 'purchase', 'active'\n",
        "        )\n",
        "    \"\"\")\n",
        "    \n",
        "    # Create templates\n",
        "    ca_templates = [\n",
        "        ('RPA', 'Residential Purchase Agreement', 'Required Sales Docs', True),\n",
        "        ('AD', 'Agency Disclosure', 'Required Sales Docs', True),\n",
        "        ('TDS', 'Transfer Disclosure Statement', 'Disclosures', True),\n",
        "        ('WFA', 'Wire Fraud Advisory', 'Optional Docs', False)\n",
        "    ]\n",
        "    \n",
        "    for code, name, folder, required in ca_templates:\n",
        "        await db.execute(\"\"\"\n",
        "            INSERT INTO document_templates (\n",
        "                office_id, state, transaction_type,\n",
        "                document_code, document_name, folder_name, is_required\n",
        "            ) VALUES ('test_office', 'CA', 'purchase', $1, $2, $3, $4)\n",
        "        \"\"\", code, name, folder, required)\n",
        "```\n",
        "\n",
        "### 5.2 Integration Test\n",
        "```python\n",
        "# tests/test_full_flow.py\n",
        "\n",
        "async def test_full_upload_flow():\n",
        "    \"\"\"Test complete flow: upload -> classify -> route -> store\"\"\"\n",
        "    \n",
        "    # 1. Upload document\n",
        "    files = [\n",
        "        (\"files\", (\"test_rpa.pdf\", open(\"test_docs/rpa.pdf\", \"rb\"), \"application/pdf\"))\n",
        "    ]\n",
        "    \n",
        "    response = requests.post(\n",
        "        \"http://localhost:8000/api/agents/upload\",\n",
        "        data={\n",
        "            \"transaction_id\": \"2025-00001\",\n",
        "            \"agent_id\": \"agent_001\"\n",
        "        },\n",
        "        files=files\n",
        "    )\n",
        "    \n",
        "    assert response.status_code == 200\n",
        "    result = response.json()\n",
        "    \n",
        "    # 2. Verify classification\n",
        "    assert result[\"successful\"] == 1\n",
        "    doc = result[\"results\"][0]\n",
        "    assert doc[\"classified_as\"] == \"Residential Purchase Agreement\"\n",
        "    assert doc[\"confidence\"] > 0.8\n",
        "    \n",
        "    # 3. Verify storage\n",
        "    document = await db.query_one(\"\"\"\n",
        "        SELECT * FROM documents WHERE document_id = $1\n",
        "    \"\"\", doc[\"document_id\"])\n",
        "    \n",
        "    assert document is not None\n",
        "    assert document[\"status\"] in [\"verified\", \"pending\"]\n",
        "    \n",
        "    # 4. Verify transaction updated\n",
        "    transaction = await db.query_one(\"\"\"\n",
        "        SELECT * FROM transactions WHERE transaction_id = '2025-00001'\n",
        "    \"\"\", transaction_id)\n",
        "    \n",
        "    assert transaction[\"total_documents\"] == 1\n",
        "    \n",
        "    print(\"âœ… Full flow test passed!\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Phase 6: Production Ready (Week 4) ğŸš€\n",
        "\n",
        "### 6.1 Add Error Handling\n",
        "```python\n",
        "# middleware/error_handler.py\n",
        "\n",
        "from fastapi import Request\n",
        "from fastapi.responses import JSONResponse\n",
        "\n",
        "@app.exception_handler(Exception)\n",
        "async def global_exception_handler(request: Request, exc: Exception):\n",
        "    \"\"\"Catch all unhandled exceptions\"\"\"\n",
        "    \n",
        "    # Log error\n",
        "    logger.error(f\"Unhandled error: {exc}\", exc_info=True)\n",
        "    \n",
        "    return JSONResponse(\n",
        "        status_code=500,\n",
        "        content={\n",
        "            \"error\": \"Internal server error\",\n",
        "            \"message\": \"Something went wrong. Please try again.\"\n",
        "        }\n",
        "    )\n",
        "```\n",
        "\n",
        "### 6.2 Add Logging\n",
        "```python\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Log important events\n",
        "logger.info(f\"Document classified: {doc_id}, confidence: {confidence}\")\n",
        "logger.warning(f\"Low confidence classification: {doc_id}, {confidence}\")\n",
        "logger.error(f\"Classification failed: {doc_id}, {error}\")\n",
        "```\n",
        "\n",
        "### 6.3 Add Rate Limiting\n",
        "```python\n",
        "from slowapi import Limiter\n",
        "from slowapi.util import get_remote_address\n",
        "\n",
        "limiter = Limiter(key_func=get_remote_address)\n",
        "\n",
        "@app.post(\"/api/agents/upload\")\n",
        "@limiter.limit(\"10/minute\")  # 10 uploads per minute per IP\n",
        "async def upload_documents(...):\n",
        "    ...\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## CRITICAL PATH SUMMARY\n",
        "\n",
        "**Must build in this order:**\n",
        "\n",
        "1. âœ… **Database schema** (Nothing works without this)\n",
        "2. âœ… **Content extractor** (Need to read files)\n",
        "3. âœ… **Classifier** (The brain - test thoroughly!)\n",
        "4. âœ… **Storage handler** (Need somewhere to put files)\n",
        "5. âœ… **Router** (Ties everything together)\n",
        "6. âœ… **Upload endpoint** (Entry point for agents)\n",
        "7. âœ… **Dashboard endpoint** (Value for brokers)\n",
        "8. âœ… **Test with real PDFs** (Catch issues early)\n",
        "9. âœ… **Production hardening** (Error handling, logging)\n",
        "\n",
        "**Don't build yet:**\n",
        "- âŒ Email ingestion (Phase 2 feature)\n",
        "- âŒ Compliance rules engine (Phase 2 feature)\n",
        "- âŒ Agent TC service ($200 add-on - Phase 3)\n",
        "- âŒ Frontend UI (Phase 2 - use Postman/curl first)\n",
        "\n",
        "**First milestone:** Agent can upload PDF via API, it gets classified, routed to correct folder, and shows in broker dashboard.\n",
        "\n",
        "Want me to create a **\"Day 1 Quick Start\"** script that sets up the absolute minimum to test the classifier?"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN3HF3AoCpSLdDGgz2H39GC",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}